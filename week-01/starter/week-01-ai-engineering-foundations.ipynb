{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: AI Engineering Mindset & Python Foundations\n",
    "\n",
    "## Overview\n",
    "Welcome to Week 1 of the AI Engineering curriculum. This week focuses on establishing an **engineering mindset** for AI development, moving beyond notebook-only approaches to building production-quality systems.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this week, you will be able to:\n",
    "- Distinguish between AI, ML, DL, and Agentic AI\n",
    "- Understand the AI system lifecycle: data → model → system → production\n",
    "- Write clean, production-quality Python code with OOP, modularity, and typing\n",
    "- Apply NumPy for efficient numerical computations\n",
    "- Process data using Pandas\n",
    "- Implement data validation and logging\n",
    "\n",
    "### Real-World Outcome\n",
    "Build a **Production Data Pipeline** that transforms raw data into clean, validated datasets with proper metrics and logging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: AI vs ML vs DL vs Agentic AI\n",
    "\n",
    "### Understanding the Landscape\n",
    "\n",
    "Let's clarify the terminology that's often used interchangeably but has distinct meanings:\n",
    "\n",
    "**Artificial Intelligence (AI)**\n",
    "- Broad field of creating systems that can perform tasks requiring human intelligence\n",
    "- Includes rule-based systems, expert systems, search algorithms, and modern ML\n",
    "\n",
    "**Machine Learning (ML)**\n",
    "- Subset of AI that learns patterns from data\n",
    "- Includes supervised, unsupervised, and reinforcement learning\n",
    "- Examples: Linear regression, decision trees, random forests\n",
    "\n",
    "**Deep Learning (DL)**\n",
    "- Subset of ML using neural networks with multiple layers\n",
    "- Excels at learning hierarchical representations\n",
    "- Examples: CNNs for vision, Transformers for language\n",
    "\n",
    "**Agentic AI**\n",
    "- AI systems that can act autonomously to achieve goals\n",
    "- Can plan, use tools, maintain memory, and adapt\n",
    "- Examples: Autonomous research agents, multi-agent systems\n",
    "\n",
    "### TODO 1.1: Map Use Cases to AI Categories\n",
    "\n",
    "Complete the function below to categorize different AI systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from enum import Enum\n",
    "\n",
    "class AICategory(Enum):\n",
    "    RULE_BASED_AI = \"rule_based_ai\"\n",
    "    TRADITIONAL_ML = \"traditional_ml\"\n",
    "    DEEP_LEARNING = \"deep_learning\"\n",
    "    AGENTIC_AI = \"agentic_ai\"\n",
    "\n",
    "def categorize_ai_system(description: str) -> AICategory:\n",
    "    \"\"\"\n",
    "    Categorize an AI system based on its description.\n",
    "    \n",
    "    Args:\n",
    "        description: Description of the AI system\n",
    "    \n",
    "    Returns:\n",
    "        AICategory enum value\n",
    "    \"\"\"\n",
    "    # TODO: Implement logic to categorize AI systems\n",
    "    # Hint: Look for keywords like \"rules\", \"neural network\", \"autonomous\", etc.\n",
    "    pass\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"A spam filter using logistic regression\",\n",
    "    \"A chess program with if-then rules\",\n",
    "    \"An image classifier using convolutional neural networks\",\n",
    "    \"An autonomous agent that plans and executes research tasks\"\n",
    "]\n",
    "\n",
    "# TODO: Uncomment and test\n",
    "# for case in test_cases:\n",
    "#     print(f\"{case}: {categorize_ai_system(case)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: AI System Lifecycle\n",
    "\n",
    "### The Pipeline: Data → Model → System → Production\n",
    "\n",
    "Real AI systems follow a structured lifecycle:\n",
    "\n",
    "1. **Data**: Collection, cleaning, validation, versioning\n",
    "2. **Model**: Training, evaluation, selection, tuning\n",
    "3. **System**: Integration, API design, error handling\n",
    "4. **Production**: Deployment, monitoring, maintenance, updates\n",
    "\n",
    "### TODO 2.1: Design a System Lifecycle Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class LifecycleStage:\n",
    "    \"\"\"Represents a stage in the AI system lifecycle.\"\"\"\n",
    "    name: str\n",
    "    started_at: Optional[datetime] = None\n",
    "    completed_at: Optional[datetime] = None\n",
    "    status: str = \"pending\"  # pending, in_progress, completed, failed\n",
    "    \n",
    "class AISystemLifecycle:\n",
    "    \"\"\"Tracks the lifecycle of an AI system.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.stages = {\n",
    "            \"data\": LifecycleStage(\"Data Collection & Preparation\"),\n",
    "            \"model\": LifecycleStage(\"Model Development\"),\n",
    "            \"system\": LifecycleStage(\"System Integration\"),\n",
    "            \"production\": LifecycleStage(\"Production Deployment\")\n",
    "        }\n",
    "    \n",
    "    def start_stage(self, stage_name: str) -> None:\n",
    "        \"\"\"Mark a stage as started.\"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # Set started_at to current time and status to \"in_progress\"\n",
    "        pass\n",
    "    \n",
    "    def complete_stage(self, stage_name: str) -> None:\n",
    "        \"\"\"Mark a stage as completed.\"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # Set completed_at to current time and status to \"completed\"\n",
    "        pass\n",
    "    \n",
    "    def get_current_stage(self) -> Optional[str]:\n",
    "        \"\"\"Get the current in-progress stage.\"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # Return the name of the stage that is \"in_progress\", or None\n",
    "        pass\n",
    "    \n",
    "    def get_progress_report(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate a progress report of all stages.\"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # Return a dict mapping stage names to their status\n",
    "        pass\n",
    "\n",
    "# TODO: Test the lifecycle tracker\n",
    "# lifecycle = AISystemLifecycle(\"Customer Churn Predictor\")\n",
    "# lifecycle.start_stage(\"data\")\n",
    "# print(lifecycle.get_progress_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Production-Quality Python\n",
    "\n",
    "### Object-Oriented Programming for AI Systems\n",
    "\n",
    "Production AI code should be:\n",
    "- **Modular**: Organized into classes and functions\n",
    "- **Typed**: Using type hints for clarity and IDE support\n",
    "- **Testable**: Easy to unit test\n",
    "- **Maintainable**: Clear naming and documentation\n",
    "\n",
    "### TODO 3.1: Build a Data Validator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Tuple\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ValidationRule:\n",
    "    \"\"\"Represents a single validation rule.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, validator: Callable[[Any], bool], error_message: str):\n",
    "        self.name = name\n",
    "        self.validator = validator\n",
    "        self.error_message = error_message\n",
    "    \n",
    "    def validate(self, value: Any) -> Tuple[bool, str]:\n",
    "        \"\"\"Run validation and return (is_valid, message).\"\"\"\n",
    "        # TODO: Implement validation logic\n",
    "        # Call self.validator(value) and return appropriate tuple\n",
    "        pass\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validates data against a set of rules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: List[ValidationRule] = []\n",
    "    \n",
    "    def add_rule(self, rule: ValidationRule) -> None:\n",
    "        \"\"\"Add a validation rule.\"\"\"\n",
    "        # TODO: Implement this method\n",
    "        pass\n",
    "    \n",
    "    def validate(self, value: Any) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate value against all rules.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_valid, list of error messages)\n",
    "        \"\"\"\n",
    "        # TODO: Implement validation against all rules\n",
    "        # Return True and empty list if all pass, False and error messages if any fail\n",
    "        pass\n",
    "\n",
    "# TODO: Create validation rules for email addresses\n",
    "# Example: email must contain '@', must have domain, etc.\n",
    "# Test with valid and invalid emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: NumPy Fundamentals & Vectorization\n",
    "\n",
    "### Why NumPy?\n",
    "- **Speed**: Vectorized operations are 10-100x faster than Python loops\n",
    "- **Memory**: Efficient array storage\n",
    "- **Foundation**: Basis for pandas, scikit-learn, TensorFlow, PyTorch\n",
    "\n",
    "### TODO 4.1: Vectorize Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(1000000)\n",
    "\n",
    "# TODO 4.1a: Implement using Python loop\n",
    "def normalize_loop(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize array to range [0, 1] using a loop.\n",
    "    Formula: (x - min) / (max - min)\n",
    "    \"\"\"\n",
    "    # TODO: Implement using a for loop\n",
    "    pass\n",
    "\n",
    "# TODO 4.1b: Implement using NumPy vectorization\n",
    "def normalize_vectorized(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize array to range [0, 1] using vectorized operations.\n",
    "    \"\"\"\n",
    "    # TODO: Implement using NumPy operations (no loops)\n",
    "    pass\n",
    "\n",
    "# TODO: Compare performance\n",
    "# Measure time for both implementations and print the speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4.2: Statistical Analysis with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(arr: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistics for an array.\n",
    "    \n",
    "    Returns dict with: mean, median, std, min, max, q25, q75\n",
    "    \"\"\"\n",
    "    # TODO: Implement statistical computations\n",
    "    # Use NumPy functions: np.mean, np.median, np.std, np.percentile, etc.\n",
    "    pass\n",
    "\n",
    "# TODO: Test with sample data\n",
    "# test_data = np.random.randn(1000)\n",
    "# stats = compute_statistics(test_data)\n",
    "# print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Pandas for Data Processing\n",
    "\n",
    "### Why Pandas?\n",
    "- **DataFrames**: Table-like data structures\n",
    "- **Missing Data**: Built-in handling of NaN values\n",
    "- **Grouping**: SQL-like operations\n",
    "- **Time Series**: Powerful date/time functionality\n",
    "\n",
    "### TODO 5.1: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample dataset\n",
    "data = {\n",
    "    'user_id': range(1, 101),\n",
    "    'age': np.random.randint(18, 70, 100),\n",
    "    'income': np.random.randint(20000, 150000, 100),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=100),\n",
    "    'is_active': np.random.choice([True, False], 100),\n",
    "    'total_purchases': np.random.randint(0, 50, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some missing values\n",
    "df.loc[np.random.choice(df.index, 10), 'income'] = np.nan\n",
    "\n",
    "def explore_dataframe(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive exploration report for a DataFrame.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - shape: (rows, columns)\n",
    "    - dtypes: dict of column types\n",
    "    - missing: dict of missing value counts per column\n",
    "    - numeric_summary: summary stats for numeric columns\n",
    "    \"\"\"\n",
    "    # TODO: Implement exploration logic\n",
    "    pass\n",
    "\n",
    "# TODO: Test the function\n",
    "# report = explore_dataframe(df)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5.2: Clean and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Handles data cleaning operations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_values(df: pd.DataFrame, strategy: str = 'mean') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values in DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            strategy: 'mean', 'median', 'mode', or 'drop'\n",
    "        \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        # TODO: Implement missing value handling\n",
    "        # Use different strategies based on the strategy parameter\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_outliers(df: pd.DataFrame, column: str, n_std: float = 3.0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove outliers from a specific column using standard deviation method.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column: Column name to check for outliers\n",
    "            n_std: Number of standard deviations for outlier threshold\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with outliers removed\n",
    "        \"\"\"\n",
    "        # TODO: Implement outlier removal\n",
    "        # Remove rows where column value is > n_std standard deviations from mean\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create derived features from existing columns.\n",
    "        \n",
    "        For the sample dataset:\n",
    "        - purchase_frequency: total_purchases / days_since_signup\n",
    "        - income_bracket: categorize income into low/medium/high\n",
    "        - age_group: categorize age into groups\n",
    "        \"\"\"\n",
    "        # TODO: Implement feature engineering\n",
    "        pass\n",
    "\n",
    "# TODO: Test the data cleaner\n",
    "# cleaner = DataCleaner()\n",
    "# df_cleaned = cleaner.handle_missing_values(df, strategy='median')\n",
    "# df_cleaned = cleaner.create_features(df_cleaned)\n",
    "# print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Data Validation & Logging\n",
    "\n",
    "### Production Data Pipelines Need:\n",
    "- **Validation**: Ensure data quality at every stage\n",
    "- **Logging**: Track operations and errors\n",
    "- **Metrics**: Measure pipeline health\n",
    "\n",
    "### TODO 6.1: Implement Pipeline Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List\n",
    "import logging\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Validates data quality in a pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.validation_results = []\n",
    "    \n",
    "    def check_missing_values(self, df: pd.DataFrame, max_missing_pct: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Check if missing values are within acceptable threshold.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to check\n",
    "            max_missing_pct: Maximum allowed percentage of missing values\n",
    "        \n",
    "        Returns:\n",
    "            True if validation passes\n",
    "        \"\"\"\n",
    "        # TODO: Implement validation\n",
    "        # Log results and return boolean\n",
    "        pass\n",
    "    \n",
    "    def check_schema(self, df: pd.DataFrame, expected_columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if DataFrame has expected columns.\n",
    "        \"\"\"\n",
    "        # TODO: Implement schema validation\n",
    "        pass\n",
    "    \n",
    "    def check_data_types(self, df: pd.DataFrame, expected_types: Dict[str, str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if columns have expected data types.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to check\n",
    "            expected_types: Dict mapping column names to expected types\n",
    "        \"\"\"\n",
    "        # TODO: Implement type validation\n",
    "        pass\n",
    "    \n",
    "    def check_value_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Check if values in a column are within expected range.\n",
    "        \"\"\"\n",
    "        # TODO: Implement range validation\n",
    "        pass\n",
    "    \n",
    "    def get_validation_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive validation report.\n",
    "        \"\"\"\n",
    "        # TODO: Compile and return validation results\n",
    "        pass\n",
    "\n",
    "# TODO: Test the quality checker\n",
    "# qc = DataQualityChecker()\n",
    "# qc.check_missing_values(df)\n",
    "# qc.check_schema(df, ['user_id', 'age', 'income'])\n",
    "# print(qc.get_validation_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Week 1 Project - Production Data Pipeline\n",
    "\n",
    "### Project Overview\n",
    "Build an end-to-end data pipeline that:\n",
    "1. Loads raw data\n",
    "2. Validates data quality\n",
    "3. Cleans and transforms data\n",
    "4. Generates metrics and logs\n",
    "5. Exports clean data\n",
    "\n",
    "### TODO 7.1: Implement Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ProductionDataPipeline:\n",
    "    \"\"\"\n",
    "    A production-grade data pipeline with validation, logging, and metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.logger = self._setup_logging()\n",
    "        self.cleaner = DataCleaner()\n",
    "        self.quality_checker = DataQualityChecker()\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Set up pipeline logging.\"\"\"\n",
    "        # TODO: Configure logging with file handler\n",
    "        pass\n",
    "    \n",
    "    def load_data(self, data_source: Any) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from source (file, database, API, etc.).\n",
    "        \"\"\"\n",
    "        # TODO: Implement data loading with error handling and logging\n",
    "        pass\n",
    "    \n",
    "    def validate_raw_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate raw data before processing.\n",
    "        \"\"\"\n",
    "        # TODO: Run multiple validation checks\n",
    "        pass\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and transform data.\n",
    "        \"\"\"\n",
    "        # TODO: Apply cleaning operations\n",
    "        pass\n",
    "    \n",
    "    def compute_metrics(self, df_raw: pd.DataFrame, df_clean: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute pipeline metrics.\n",
    "        \n",
    "        Metrics to compute:\n",
    "        - rows_processed\n",
    "        - rows_removed\n",
    "        - missing_values_handled\n",
    "        - processing_time\n",
    "        - data_quality_score\n",
    "        \"\"\"\n",
    "        # TODO: Implement metrics computation\n",
    "        pass\n",
    "    \n",
    "    def export_data(self, df: pd.DataFrame, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Export cleaned data to file.\n",
    "        \"\"\"\n",
    "        # TODO: Implement data export with error handling\n",
    "        pass\n",
    "    \n",
    "    def run(self, data_source: Any, output_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the complete pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Pipeline execution report\n",
    "        \"\"\"\n",
    "        # TODO: Orchestrate the complete pipeline\n",
    "        # 1. Load data\n",
    "        # 2. Validate raw data\n",
    "        # 3. Clean data\n",
    "        # 4. Compute metrics\n",
    "        # 5. Export data\n",
    "        # 6. Return execution report\n",
    "        pass\n",
    "\n",
    "# TODO: Test the complete pipeline\n",
    "# pipeline = ProductionDataPipeline(\"user_data_pipeline\")\n",
    "# report = pipeline.run(df, \"cleaned_data.csv\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "- Distinctions between AI, ML, DL, and Agentic AI\n",
    "- AI system lifecycle from data to production\n",
    "- Production-quality Python with OOP and typing\n",
    "- NumPy for efficient numerical computations\n",
    "- Pandas for data manipulation\n",
    "- Data validation and logging practices\n",
    "- Building a complete production data pipeline\n",
    "\n",
    "### Real-World Applications\n",
    "- Data engineering pipelines\n",
    "- ETL processes\n",
    "- ML data preparation\n",
    "- Data quality monitoring systems\n",
    "\n",
    "### Next Week Preview\n",
    "**Week 2: Probability, Uncertainty & Decision Systems**\n",
    "- Build explainable decision-making systems\n",
    "- Apply probability in real-world scenarios\n",
    "- Create a risk scoring engine\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Practice\n",
    "\n",
    "### Challenge 1: Enhanced Pipeline\n",
    "Extend the pipeline to:\n",
    "- Handle multiple data sources\n",
    "- Implement data versioning\n",
    "- Add pipeline scheduling\n",
    "\n",
    "### Challenge 2: Real Dataset\n",
    "Apply your pipeline to a real dataset:\n",
    "- Download a dataset from Kaggle or UCI ML Repository\n",
    "- Process it through your pipeline\n",
    "- Generate a quality report\n",
    "\n",
    "### Challenge 3: Unit Tests\n",
    "Write unit tests for:\n",
    "- DataValidator class\n",
    "- DataCleaner methods\n",
    "- DataQualityChecker validations\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
