{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Large Language Model Systems\n\n",
    "## Overview\n",
    "Welcome to Week 7 - Large Language Model Systems. This week focuses on working with LLMs through APIs, prompt engineering, and building production-ready LLM applications.\n\n",
    "### Learning Objectives\n",
    "- Understand LLM architecture and capabilities\n",
    "- Work with LLM APIs (OpenAI, Anthropic, etc.)\n",
    "- Master prompt engineering patterns\n",
    "- Understand prompt vs fine-tuning tradeoffs\n",
    "- Implement output evaluation\n",
    "- Optimize for cost and latency\n\n",
    "### Real-World Outcome\n",
    "Build an **AI Writing & Analysis Assistant** that can generate, analyze, and improve text content.\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import logging\n",
    "\n",
    "# LLM libraries\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "# Utilities\n",
    "import tiktoken\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize clients\n",
    "# client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Basics\n\n",
    "### TODO 1.1: Implement LLM Client Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"Wrapper for LLM API calls with error handling and retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'gpt-3.5-turbo', api_key: Optional[str] = None):\n",
    "        # TODO: Initialize client\n",
    "        self.model = model\n",
    "        self.client = None  # OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))\n",
    "        self.tokenizer = None  # tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        # TODO: Implement token counting\n",
    "        pass\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))\n",
    "    def complete(self, prompt: str, temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "        \"\"\"Generate completion with retry logic.\"\"\"\n",
    "        # TODO: Call API\n",
    "        pass\n",
    "    \n",
    "    def chat_complete(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "        \"\"\"Chat completion with message history.\"\"\"\n",
    "        # TODO: Implement chat completion\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prompt Engineering\n\n",
    "### TODO 2.1: Implement Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    \"\"\"Reusable prompt templates with variable substitution.\"\"\"\n",
    "    \n",
    "    def __init__(self, template: str, variables: List[str]):\n",
    "        self.template = template\n",
    "        self.variables = variables\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"Format template with variables.\"\"\"\n",
    "        # TODO: Validate and format\n",
    "        pass\n",
    "\n",
    "class PromptLibrary:\n",
    "    \"\"\"Library of prompt engineering patterns.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_shot(task: str, input_text: str) -> str:\n",
    "        \"\"\"Zero-shot prompting pattern.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot(task: str, examples: List[Tuple[str, str]], input_text: str) -> str:\n",
    "        \"\"\"Few-shot prompting with examples.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(problem: str) -> str:\n",
    "        \"\"\"Chain-of-thought prompting for reasoning.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Output Evaluation\n\n",
    "### TODO 3.1: Implement Output Quality Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEvaluator:\n",
    "    \"\"\"Evaluate LLM output quality.\"\"\"\n",
    "    \n",
    "    def check_relevance(self, output: str, query: str) -> float:\n",
    "        \"\"\"Check if output is relevant to query.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_coherence(self, output: str) -> float:\n",
    "        \"\"\"Check output coherence.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_factuality(self, output: str, reference: Optional[str] = None) -> float:\n",
    "        \"\"\"Check factuality against reference.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: AI Writing Assistant Project\n\n",
    "### TODO 4.1: Build Complete Writing Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIWritingAssistant:\n",
    "    \"\"\"Complete AI writing and analysis assistant.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'gpt-3.5-turbo'):\n",
    "        self.client = LLMClient(model)\n",
    "        self.evaluator = OutputEvaluator()\n",
    "    \n",
    "    def generate_content(self, topic: str, style: str = 'professional', length: int = 500) -> str:\n",
    "        \"\"\"Generate content on topic.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def improve_text(self, text: str, aspect: str = 'clarity') -> str:\n",
    "        \"\"\"Improve text for specific aspect.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def analyze_text(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze text quality and provide feedback.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def summarize(self, text: str, max_length: int = 100) -> str:\n",
    "        \"\"\"Summarize text.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n",
    "This week you learned:\n",
    "- LLM API usage and best practices\n",
    "- Prompt engineering patterns\n",
    "- Output evaluation techniques\n",
    "- Cost and latency optimization\n",
    "- Building production LLM applications\n\n",
    "Next week: Retrieval-Augmented Generation (RAG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}