{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: NLP & Transformers (Solution)\n",
    "\n",
    "Complete solution implementations for Week 6 exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Preprocessing - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, lowercase=True, remove_punctuation=True, remove_stopwords=True, lemmatize=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        if lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        text = self.clean_text(text)\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        tokens = self.tokenize(text)\n",
    "        if self.remove_punctuation:\n",
    "            tokens = [t for t in tokens if t not in string.punctuation]\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stopwords]\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_documents(self, documents: List[str]) -> List[List[str]]:\n",
    "        return [self.preprocess(doc) for doc in documents]\n",
    "\n",
    "# Test\n",
    "preprocessor = TextPreprocessor()\n",
    "test_text = \"Check out https://example.com! Email: test@email.com. This is GREAT!!!\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {preprocessor.preprocess(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Vectorization - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer:\n",
    "    def __init__(self, max_features: int = 5000):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
    "    \n",
    "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
    "        return self.vectorizer.fit_transform(documents).toarray()\n",
    "    \n",
    "    def transform(self, documents: List[str]) -> np.ndarray:\n",
    "        return self.vectorizer.transform(documents).toarray()\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        return self.vectorizer.get_feature_names_out().tolist()\n",
    "    \n",
    "    def get_top_features(self, document_vector: np.ndarray, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        feature_names = self.get_feature_names()\n",
    "        top_indices = np.argsort(document_vector)[-top_n:][::-1]\n",
    "        return [(feature_names[i], document_vector[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Embeddings - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "    def __init__(self, model_name: str = 'bert-base-uncased'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def get_sentence_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        return embedding\n",
    "    \n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        emb1 = self.get_sentence_embedding(text1)\n",
    "        emb2 = self.get_sentence_embedding(text2)\n",
    "        return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attention Mechanism - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.hidden_size)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: NLP Evaluator - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPEvaluator:\n",
    "    @staticmethod\n",
    "    def evaluate_classification(y_true: List[int], y_pred: List[int], class_names: List[str] = None) -> Dict:\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'report': classification_report(y_true, y_pred, target_names=class_names)\n",
    "        }\n",
    "        print(metrics['report'])\n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true: List[int], y_pred: List[int], class_names: List[str]):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Document Intelligence Engine - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIntelligenceEngine:\n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased'):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.embedding_handler = EmbeddingHandler(model_name)\n",
    "        self.sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "        self.summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "        self.ner = pipeline('ner', aggregation_strategy='simple')\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> Dict:\n",
    "        return self.sentiment_analyzer(text[:512])[0]\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        vectorizer = TextVectorizer(max_features=100)\n",
    "        tfidf_matrix = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_top_features(tfidf_matrix[0], top_n)\n",
    "    \n",
    "    def summarize(self, text: str, max_length: int = 150) -> str:\n",
    "        if len(text.split()) < 50:\n",
    "            return text\n",
    "        summary = self.summarizer(text[:1024], max_length=max_length, min_length=30, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        return self.ner(text[:512])\n",
    "    \n",
    "    def find_similar_documents(self, query: str, documents: List[str], top_k: int = 5) -> List[int]:\n",
    "        query_emb = self.embedding_handler.get_sentence_embedding(query)\n",
    "        doc_embs = [self.embedding_handler.get_sentence_embedding(doc[:512]) for doc in documents]\n",
    "        similarities = [np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)) \n",
    "                       for doc_emb in doc_embs]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return top_indices.tolist()\n",
    "    \n",
    "    def analyze_document(self, text: str) -> Dict:\n",
    "        return {\n",
    "            'sentiment': self.analyze_sentiment(text),\n",
    "            'keywords': self.extract_keywords(text),\n",
    "            'summary': self.summarize(text),\n",
    "            'entities': self.extract_entities(text)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize engine\n",
    "engine = DocumentIntelligenceEngine()\n",
    "\n",
    "# Sample document\n",
    "sample_doc = \"\"\"\n",
    "Apple Inc. announced record quarterly earnings today, with CEO Tim Cook praising \n",
    "the strong performance of iPhone sales globally. The tech giant's stock surged \n",
    "5% in after-hours trading as investors reacted positively to the news.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze\n",
    "results = engine.analyze_document(sample_doc)\n",
    "print(\"Analysis Results:\")\n",
    "print(f\"Sentiment: {results['sentiment']}\")\n",
    "print(f\"Keywords: {results['keywords'][:5]}\")\n",
    "print(f\"Entities: {results['entities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This solution demonstrates complete implementations of:\n",
    "- Text preprocessing pipelines\n",
    "- TF-IDF vectorization\n",
    "- Transformer-based embeddings\n",
    "- Attention mechanisms\n",
    "- Document intelligence system\n",
    "- Sentiment analysis, summarization, and NER\n",
    "\n",
    "Use these as reference implementations for production NLP systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
