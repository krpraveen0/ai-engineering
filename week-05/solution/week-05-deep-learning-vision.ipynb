{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Deep Learning Essentials & Vision (Solution)\n",
    "\n",
    "## Overview\n",
    "This notebook contains complete solutions for Week 5. Use this as reference after attempting the starter notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Neural Network Fundamentals - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralLayer:\n",
    "    \"\"\"Simple neural network layer with forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        # Initialize weights with small random values scaled by sqrt(input_size)\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        # Initialize bias as zeros\n",
    "        self.bias = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: compute output = x @ weights + bias\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply ReLU activation: max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "# Test the layer\n",
    "layer = SimpleNeuralLayer(10, 5)\n",
    "test_input = np.random.randn(3, 10)\n",
    "output = layer.forward(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output with ReLU shape: {layer.relu(output).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, dropout: float = 0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Additional hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "# Test the MLP\n",
    "model = MLPClassifier(input_size=784, hidden_sizes=[256, 128], num_classes=10)\n",
    "print(model)\n",
    "test_input = torch.randn(32, 784)\n",
    "output = model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Convolutional Neural Networks - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    \"\"\"Basic CNN for image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # For 32x32 input: after 3 pooling layers â†’ 4x4 spatial size\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Conv block 1\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = self.pool3(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the CNN\n",
    "model = BasicCNN(num_classes=10, input_channels=3)\n",
    "print(model)\n",
    "test_input = torch.randn(8, 3, 32, 32)\n",
    "output = model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Data Augmentation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset with augmentation support.\"\"\"\n",
    "    \n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_data_transforms(train: bool = True):\n",
    "    \"\"\"Get data augmentation transforms.\"\"\"\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Pipeline - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Trainer class for CNN models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, criterion, optimizer) -> Tuple[float, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader: DataLoader, criterion) -> Tuple[float, float]:\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = running_loss / len(val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "              num_epochs: int, learning_rate: float = 0.001):\n",
    "        \"\"\"Full training pipeline.\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                       f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "                       f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation metrics.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        ax1.plot(self.train_losses, label='Train Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Transfer Learning - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningModel:\n",
    "    \"\"\"Transfer learning wrapper using pre-trained models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_classes: int, freeze_layers: bool = True):\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            num_features = self.model.fc.in_features\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "            num_features = self.model.fc.in_features\n",
    "        elif model_name == 'vgg16':\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            num_features = self.model.classifier[6].in_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        if freeze_layers:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        if 'resnet' in model_name:\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        elif 'vgg' in model_name:\n",
    "            self.model.classifier[6] = nn.Sequential(\n",
    "                nn.Linear(num_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "    \n",
    "    def get_model(self) -> nn.Module:\n",
    "        return self.model\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers: int = None):\n",
    "        \"\"\"Unfreeze layers for fine-tuning.\"\"\"\n",
    "        if num_layers is None:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Unfreeze last num_layers\n",
    "            params = list(self.model.parameters())\n",
    "            for param in params[-num_layers:]:\n",
    "                param.requires_grad = True\n",
    "\n",
    "# Test transfer learning\n",
    "tl_model = TransferLearningModel('resnet18', num_classes=5, freeze_layers=True)\n",
    "model = tl_model.get_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Regularization & Early Stopping - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.0, verbose: bool = True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"Initial validation loss: {val_loss:.4f}\")\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved to {val_loss:.4f}\")\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No improvement for {self.counter} epoch(s)\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visual Defect Detection System - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDetectionSystem:\n",
    "    \"\"\"Complete system for visual defect detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'cnn', use_transfer_learning: bool = False):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "        if use_transfer_learning:\n",
    "            tl_model = TransferLearningModel('resnet18', num_classes=2, freeze_layers=True)\n",
    "            self.model = tl_model.get_model()\n",
    "        else:\n",
    "            self.model = BasicCNN(num_classes=2, input_channels=3)\n",
    "    \n",
    "    def prepare_data(self, images: np.ndarray, labels: np.ndarray, \n",
    "                     test_size: float = 0.2, val_size: float = 0.1, batch_size: int = 32):\n",
    "        \"\"\"Load and prepare data.\"\"\"\n",
    "        # Split data\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, labels, test_size=test_size, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = ImageDataset(X_train, y_train, transform=get_data_transforms(train=True))\n",
    "        val_dataset = ImageDataset(X_val, y_val, transform=get_data_transforms(train=False))\n",
    "        test_dataset = ImageDataset(X_test, y_test, transform=get_data_transforms(train=False))\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs: int = 50, learning_rate: float = 0.001):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        self.trainer = ModelTrainer(self.model, device)\n",
    "        self.trainer.train(train_loader, val_loader, num_epochs, learning_rate)\n",
    "    \n",
    "    def evaluate(self, test_loader) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model on test set.\"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, average='binary'),\n",
    "            'recall': recall_score(all_labels, all_preds, average='binary'),\n",
    "            'f1': f1_score(all_labels, all_preds, average='binary')\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> Tuple[str, float]:\n",
    "        \"\"\"Predict on single image.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Prepare image\n",
    "        transform = get_data_transforms(train=False)\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        image_pil = Image.fromarray(image)\n",
    "        image_tensor = transform(image_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            confidence, predicted = torch.max(probabilities, 1)\n",
    "        \n",
    "        class_names = ['Non-Defective', 'Defective']\n",
    "        prediction = class_names[predicted.item()]\n",
    "        confidence = confidence.item()\n",
    "        \n",
    "        return prediction, confidence\n",
    "    \n",
    "    def visualize_predictions(self, images: np.ndarray, labels: np.ndarray, num_samples: int = 8):\n",
    "        \"\"\"Visualize predictions on sample images.\"\"\"\n",
    "        indices = np.random.choice(len(images), min(num_samples, len(images)), replace=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        class_names = ['Non-Defective', 'Defective']\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            image = images[idx]\n",
    "            true_label = labels[idx]\n",
    "            prediction, confidence = self.predict(image)\n",
    "            \n",
    "            axes[i].imshow(image)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f\"True: {class_names[true_label]}\\nPred: {prediction} ({confidence:.2f})\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Evaluation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation utilities.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str] = None) -> Dict:\n",
    "        \"\"\"Compute comprehensive metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str]):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_misclassifications(images: np.ndarray, y_true: np.ndarray, \n",
    "                                     y_pred: np.ndarray, class_names: List[str], num_samples: int = 9):\n",
    "        \"\"\"Visualize misclassified samples.\"\"\"\n",
    "        misclassified_idx = np.where(y_true != y_pred)[0]\n",
    "        \n",
    "        if len(misclassified_idx) == 0:\n",
    "            print(\"No misclassifications!\")\n",
    "            return\n",
    "        \n",
    "        indices = np.random.choice(misclassified_idx, min(num_samples, len(misclassified_idx)), replace=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            axes[i].imshow(images[idx])\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f\"True: {class_names[y_true[idx]]}\\nPred: {class_names[y_pred[idx]]}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Model Persistence - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    \"\"\"Save and load model checkpoints.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(model: nn.Module, path: str, metadata: Dict = None):\n",
    "        \"\"\"Save model with metadata.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(model: nn.Module, path: str) -> nn.Module:\n",
    "        \"\"\"Load model from checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "        \n",
    "        if 'metadata' in checkpoint:\n",
    "            logger.info(f\"Metadata: {checkpoint['metadata']}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Example usage\n",
    "# ModelCheckpoint.save_model(model, 'defect_detector.pth', metadata={'accuracy': 0.95, 'epoch': 50})\n",
    "# loaded_model = ModelCheckpoint.load_model(BasicCNN(num_classes=2), 'defect_detector.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Complete Workflow with CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Create model\n",
    "model = BasicCNN(num_classes=10, input_channels=3)\n",
    "\n",
    "# Train model\n",
    "trainer = ModelTrainer(model, device)\n",
    "trainer.train(train_loader, test_loader, num_epochs=10, learning_rate=0.001)\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_training_history()\n",
    "\n",
    "# Save model\n",
    "ModelCheckpoint.save_model(model, 'cifar10_cnn.pth', metadata={'accuracy': trainer.val_accuracies[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This solution notebook demonstrates:\n",
    "1. Complete implementations of neural networks and CNNs\n",
    "2. Data augmentation and preprocessing pipelines\n",
    "3. Training loops with proper evaluation\n",
    "4. Transfer learning implementation\n",
    "5. Regularization techniques including early stopping\n",
    "6. Complete defect detection system\n",
    "7. Model evaluation and visualization\n",
    "8. Model persistence and checkpointing\n",
    "\n",
    "Use this as reference after completing the starter notebook exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
