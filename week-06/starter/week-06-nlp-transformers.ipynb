{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: NLP & Transformers\n",
    "\n",
    "## Overview\n",
    "Welcome to Week 6 of the AI Engineering curriculum. This week focuses on **Natural Language Processing (NLP)** and the **Transformer architecture**, which revolutionized how machines understand and generate human language.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this week, you will be able to:\n",
    "- Understand text preprocessing and tokenization strategies\n",
    "- Work with word and sentence embeddings\n",
    "- Comprehend the Transformer architecture and attention mechanism\n",
    "- Build NLP pipelines using modern pre-trained models\n",
    "- Apply transformers to text classification and analysis tasks\n",
    "- Evaluate NLP systems properly\n",
    "\n",
    "### Real-World Outcome\n",
    "Build a **Document Intelligence Engine** that can classify, extract insights, and analyze text documents automatically.\n",
    "\n",
    "### Prerequisites\n",
    "- Python fundamentals (Week 1)\n",
    "- Machine Learning basics (Week 4)\n",
    "- Deep Learning fundamentals (Week 5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, pipeline\n",
    ")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Text Preprocessing\n",
    "\n",
    "### 1.1 Understanding Text Preprocessing\n",
    "\n",
    "Text preprocessing converts raw text into a clean, structured format suitable for machine learning.\n",
    "\n",
    "**Common Steps:**\n",
    "- **Lowercasing**: Normalize case\n",
    "- **Tokenization**: Split text into words/tokens\n",
    "- **Punctuation Removal**: Remove special characters\n",
    "- **Stopword Removal**: Remove common words (the, is, at)\n",
    "- **Stemming/Lemmatization**: Reduce words to root form\n",
    "- **Cleaning**: Remove URLs, emails, numbers, etc.\n",
    "\n",
    "### TODO 1.1: Implement Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = True,\n",
    "                 remove_stopwords: bool = True,\n",
    "                 lemmatize: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with options.\n",
    "        \n",
    "        Args:\n",
    "            lowercase: Convert text to lowercase\n",
    "            remove_punctuation: Remove punctuation marks\n",
    "            remove_stopwords: Remove common stopwords\n",
    "            lemmatize: Apply lemmatization (vs stemming)\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "        # TODO: Initialize stopwords set\n",
    "        self.stopwords = None  # Replace with: set(stopwords.words('english'))\n",
    "        \n",
    "        # TODO: Initialize lemmatizer or stemmer\n",
    "        if lemmatize:\n",
    "            self.lemmatizer = None  # Replace with: WordNetLemmatizer()\n",
    "        else:\n",
    "            self.stemmer = None  # Replace with: PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text by removing URLs, emails, numbers, and extra whitespace.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "        \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # TODO: Remove URLs (http://... or https://...)\n",
    "        # Hint: Use re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # TODO: Remove email addresses\n",
    "        # Hint: Use re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # TODO: Remove numbers\n",
    "        # Hint: Use re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # TODO: Remove extra whitespace\n",
    "        # Hint: Use re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into words.\n",
    "        \n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        # TODO: Use NLTK's word_tokenize\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            List of processed tokens\n",
    "        \"\"\"\n",
    "        # TODO: Clean text\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # TODO: Convert to lowercase if enabled\n",
    "        if self.lowercase:\n",
    "            pass\n",
    "        \n",
    "        # TODO: Tokenize\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # TODO: Remove punctuation if enabled\n",
    "        if self.remove_punctuation:\n",
    "            # Filter out tokens that are pure punctuation\n",
    "            pass\n",
    "        \n",
    "        # TODO: Remove stopwords if enabled\n",
    "        if self.remove_stopwords:\n",
    "            pass\n",
    "        \n",
    "        # TODO: Lemmatize or stem if enabled\n",
    "        if self.lemmatize:\n",
    "            # tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            pass\n",
    "        else:\n",
    "            # tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "            pass\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_documents(self, documents: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Preprocess multiple documents.\n",
    "        \n",
    "        Returns:\n",
    "            List of token lists\n",
    "        \"\"\"\n",
    "        # TODO: Process each document\n",
    "        pass\n",
    "\n",
    "# Test the preprocessor\n",
    "# TODO: Uncomment and test\n",
    "# preprocessor = TextPreprocessor()\n",
    "# test_text = \"Check out https://example.com for more info! Email me at test@email.com. This is GREAT!!!\"\n",
    "# tokens = preprocessor.preprocess(test_text)\n",
    "# print(f\"Original: {test_text}\")\n",
    "# print(f\"Processed: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Text Representation & Embeddings\n",
    "\n",
    "### 2.1 Understanding Text Representations\n",
    "\n",
    "**Traditional Methods:**\n",
    "- **Bag of Words (BoW)**: Count word occurrences\n",
    "- **TF-IDF**: Weight by term frequency and inverse document frequency\n",
    "\n",
    "**Modern Methods:**\n",
    "- **Word Embeddings**: Dense vectors capturing semantic meaning (Word2Vec, GloVe)\n",
    "- **Contextual Embeddings**: Context-aware representations (BERT, GPT)\n",
    "\n",
    "### TODO 2.1: Implement TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer:\n",
    "    \"\"\"\n",
    "    Text vectorization using TF-IDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_features: int = 5000):\n",
    "        \"\"\"\n",
    "        Initialize vectorizer.\n",
    "        \n",
    "        Args:\n",
    "            max_features: Maximum number of features to keep\n",
    "        \"\"\"\n",
    "        # TODO: Initialize TfidfVectorizer\n",
    "        # Set max_features, ngram_range=(1, 2) for unigrams and bigrams\n",
    "        self.vectorizer = None\n",
    "    \n",
    "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit vectorizer and transform documents.\n",
    "        \n",
    "        Returns:\n",
    "            TF-IDF matrix\n",
    "        \"\"\"\n",
    "        # TODO: Fit and transform using self.vectorizer\n",
    "        pass\n",
    "    \n",
    "    def transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform documents using fitted vectorizer.\n",
    "        \"\"\"\n",
    "        # TODO: Transform using self.vectorizer\n",
    "        pass\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get feature names (vocabulary).\n",
    "        \"\"\"\n",
    "        # TODO: Return feature names\n",
    "        pass\n",
    "    \n",
    "    def get_top_features(self, document_vector: np.ndarray, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get top N features for a document.\n",
    "        \n",
    "        Returns:\n",
    "            List of (feature, score) tuples\n",
    "        \"\"\"\n",
    "        # TODO: Get indices of top N values\n",
    "        # Get corresponding feature names and scores\n",
    "        pass\n",
    "\n",
    "# Test the vectorizer\n",
    "# TODO: Test with sample documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embeddings\n",
    "\n",
    "Word embeddings map words to dense vectors where semantic similarity is captured by vector proximity.\n",
    "\n",
    "### TODO 2.2: Work with Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "    \"\"\"\n",
    "    Handle word embeddings using pre-trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'bert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Initialize with pre-trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of Hugging Face model\n",
    "        \"\"\"\n",
    "        # TODO: Load tokenizer and model\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def get_sentence_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get embedding for a sentence.\n",
    "        \n",
    "        Returns:\n",
    "            Embedding vector\n",
    "        \"\"\"\n",
    "        # TODO: Tokenize text\n",
    "        # inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # TODO: Get model output\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = self.model(**inputs)\n",
    "        \n",
    "        # TODO: Use [CLS] token embedding or mean pooling\n",
    "        # embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts.\n",
    "        \n",
    "        Returns:\n",
    "            Similarity score (0-1)\n",
    "        \"\"\"\n",
    "        # TODO: Get embeddings for both texts\n",
    "        # TODO: Compute cosine similarity\n",
    "        # cosine_sim = (emb1 @ emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        pass\n",
    "\n",
    "# Test embedding handler\n",
    "# TODO: Test with sample texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Transformer Architecture\n",
    "\n",
    "### 3.1 Understanding Transformers\n",
    "\n",
    "**Key Components:**\n",
    "- **Self-Attention**: Each token attends to all other tokens\n",
    "- **Multi-Head Attention**: Multiple attention mechanisms in parallel\n",
    "- **Position Encoding**: Inject sequence order information\n",
    "- **Feed-Forward Networks**: Process attended representations\n",
    "- **Layer Normalization**: Stabilize training\n",
    "\n",
    "**Why Transformers?**\n",
    "- Parallel processing (unlike RNNs)\n",
    "- Long-range dependencies\n",
    "- Transfer learning capability\n",
    "\n",
    "### TODO 3.1: Implement Simple Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple scaled dot-product attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # TODO: Define query, key, value projections\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, hidden_size)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Attended output\n",
    "        \"\"\"\n",
    "        # TODO: Compute Q, K, V\n",
    "        # Q = self.query(x)\n",
    "        # K = self.key(x)\n",
    "        # V = self.value(x)\n",
    "        \n",
    "        # TODO: Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "        # scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.hidden_size)\n",
    "        \n",
    "        # TODO: Apply mask if provided\n",
    "        # if mask is not None:\n",
    "        #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # TODO: Apply softmax\n",
    "        # attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # TODO: Apply attention to values\n",
    "        # output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test attention\n",
    "# TODO: Test with sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using Pre-trained Transformers\n",
    "\n",
    "Pre-trained transformer models like BERT, RoBERTa, and DistilBERT can be fine-tuned for specific tasks.\n",
    "\n",
    "### TODO 3.2: Build Text Classifier with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier:\n",
    "    \"\"\"\n",
    "    Text classification using pre-trained transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased', num_labels: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize classifier.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Pre-trained model name\n",
    "            num_labels: Number of classification labels\n",
    "        \"\"\"\n",
    "        # TODO: Load tokenizer\n",
    "        self.tokenizer = None  # AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # TODO: Load model for sequence classification\n",
    "        self.model = None  # AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    def prepare_dataset(self, texts: List[str], labels: List[int]) -> Dataset:\n",
    "        \"\"\"\n",
    "        Prepare dataset for training.\n",
    "        \n",
    "        Returns:\n",
    "            PyTorch Dataset\n",
    "        \"\"\"\n",
    "        # TODO: Tokenize texts\n",
    "        # encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # TODO: Create dataset\n",
    "        # class TextDataset(Dataset):\n",
    "        #     def __init__(self, encodings, labels):\n",
    "        #         self.encodings = encodings\n",
    "        #         self.labels = labels\n",
    "        #     \n",
    "        #     def __len__(self):\n",
    "        #         return len(self.labels)\n",
    "        #     \n",
    "        #     def __getitem__(self, idx):\n",
    "        #         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        #         item['labels'] = torch.tensor(self.labels[idx])\n",
    "        #         return item\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def train(self, train_dataset: Dataset, val_dataset: Dataset, \n",
    "              output_dir: str = './results', num_epochs: int = 3):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        # TODO: Define training arguments\n",
    "        # training_args = TrainingArguments(\n",
    "        #     output_dir=output_dir,\n",
    "        #     num_train_epochs=num_epochs,\n",
    "        #     per_device_train_batch_size=16,\n",
    "        #     per_device_eval_batch_size=16,\n",
    "        #     warmup_steps=500,\n",
    "        #     weight_decay=0.01,\n",
    "        #     logging_dir='./logs',\n",
    "        #     evaluation_strategy=\"epoch\",\n",
    "        #     save_strategy=\"epoch\",\n",
    "        #     load_best_model_at_end=True\n",
    "        # )\n",
    "        \n",
    "        # TODO: Create trainer\n",
    "        # trainer = Trainer(\n",
    "        #     model=self.model,\n",
    "        #     args=training_args,\n",
    "        #     train_dataset=train_dataset,\n",
    "        #     eval_dataset=val_dataset\n",
    "        # )\n",
    "        \n",
    "        # TODO: Train\n",
    "        # trainer.train()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Predict labels for texts.\n",
    "        \n",
    "        Returns:\n",
    "            List of predictions with labels and scores\n",
    "        \"\"\"\n",
    "        # TODO: Create pipeline for inference\n",
    "        # classifier = pipeline('text-classification', model=self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "        # TODO: Predict\n",
    "        # predictions = classifier(texts)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test transformer classifier\n",
    "# TODO: Test with sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: NLP Evaluation\n",
    "\n",
    "### 4.1 Evaluation Metrics for NLP\n",
    "\n",
    "**Classification Metrics:**\n",
    "- Accuracy, Precision, Recall, F1-score\n",
    "- Per-class metrics\n",
    "- Confusion matrix\n",
    "\n",
    "**Text Generation Metrics:**\n",
    "- BLEU, ROUGE, METEOR\n",
    "- Perplexity\n",
    "\n",
    "### TODO 4.1: Implement NLP Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation utilities for NLP tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_classification(y_true: List[int], y_pred: List[int], \n",
    "                               class_names: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate classification performance.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        # TODO: Compute metrics\n",
    "        metrics = {\n",
    "            'accuracy': None,  # accuracy_score(y_true, y_pred)\n",
    "            'report': None  # classification_report(y_true, y_pred, target_names=class_names)\n",
    "        }\n",
    "        \n",
    "        # TODO: Print report\n",
    "        # print(metrics['report'])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true: List[int], y_pred: List[int], class_names: List[str]):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix.\n",
    "        \"\"\"\n",
    "        # TODO: Compute confusion matrix\n",
    "        # cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # TODO: Plot heatmap\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_errors(texts: List[str], y_true: List[int], \n",
    "                       y_pred: List[int], class_names: List[str], num_samples: int = 5):\n",
    "        \"\"\"\n",
    "        Analyze and display misclassified samples.\n",
    "        \"\"\"\n",
    "        # TODO: Find misclassified indices\n",
    "        # misclassified = np.where(np.array(y_true) != np.array(y_pred))[0]\n",
    "        \n",
    "        # TODO: Sample and display\n",
    "        pass\n",
    "\n",
    "# Test evaluator\n",
    "# TODO: Test with sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Document Intelligence Engine (Project)\n",
    "\n",
    "### 5.1 Problem Statement\n",
    "\n",
    "Build a system that can:\n",
    "- Classify documents by category\n",
    "- Extract key information\n",
    "- Analyze sentiment\n",
    "- Summarize content\n",
    "\n",
    "### TODO 5.1: Build Document Intelligence System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIntelligenceEngine:\n",
    "    \"\"\"\n",
    "    Complete document intelligence system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Initialize the engine.\n",
    "        \"\"\"\n",
    "        # TODO: Initialize components\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.classifier = None  # TransformerClassifier(model_name)\n",
    "        self.embedding_handler = None  # EmbeddingHandler(model_name)\n",
    "        \n",
    "        # TODO: Initialize additional pipelines\n",
    "        # self.sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "        # self.summarizer = pipeline('summarization')\n",
    "    \n",
    "    def classify_document(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Classify document category.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with label and confidence\n",
    "        \"\"\"\n",
    "        # TODO: Preprocess and classify\n",
    "        pass\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze document sentiment.\n",
    "        \n",
    "        Returns:\n",
    "            Sentiment label and score\n",
    "        \"\"\"\n",
    "        # TODO: Use sentiment analyzer\n",
    "        pass\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract key terms from document.\n",
    "        \n",
    "        Returns:\n",
    "            List of (keyword, importance) tuples\n",
    "        \"\"\"\n",
    "        # TODO: Use TF-IDF or other method\n",
    "        pass\n",
    "    \n",
    "    def summarize(self, text: str, max_length: int = 150) -> str:\n",
    "        \"\"\"\n",
    "        Generate document summary.\n",
    "        \n",
    "        Returns:\n",
    "            Summary text\n",
    "        \"\"\"\n",
    "        # TODO: Use summarization pipeline\n",
    "        pass\n",
    "    \n",
    "    def find_similar_documents(self, query: str, documents: List[str], top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Find most similar documents to query.\n",
    "        \n",
    "        Returns:\n",
    "            Indices of top-k similar documents\n",
    "        \"\"\"\n",
    "        # TODO: Compute embeddings and similarity\n",
    "        pass\n",
    "    \n",
    "    def analyze_document(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete document analysis.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all analysis results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'classification': self.classify_document(text),\n",
    "            'sentiment': self.analyze_sentiment(text),\n",
    "            'keywords': self.extract_keywords(text),\n",
    "            'summary': self.summarize(text)\n",
    "        }\n",
    "        return results\n",
    "\n",
    "# TODO: Build and test the complete system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Advanced NLP Techniques\n",
    "\n",
    "### 6.1 Named Entity Recognition (NER)\n",
    "\n",
    "### TODO 6.1: Implement NER System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERSystem:\n",
    "    \"\"\"\n",
    "    Named Entity Recognition system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Load NER pipeline\n",
    "        # self.ner = pipeline('ner', aggregation_strategy='simple')\n",
    "        self.ner = None\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract named entities from text.\n",
    "        \n",
    "        Returns:\n",
    "            List of entities with type and score\n",
    "        \"\"\"\n",
    "        # TODO: Use NER pipeline\n",
    "        pass\n",
    "    \n",
    "    def visualize_entities(self, text: str):\n",
    "        \"\"\"\n",
    "        Visualize entities in text.\n",
    "        \"\"\"\n",
    "        # TODO: Extract and highlight entities\n",
    "        pass\n",
    "\n",
    "# Test NER system\n",
    "# TODO: Test with sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### What You Learned This Week\n",
    "\n",
    "1. **Text Preprocessing**\n",
    "   - Tokenization, cleaning, normalization\n",
    "   - Stopword removal and lemmatization\n",
    "   - Building preprocessing pipelines\n",
    "\n",
    "2. **Text Representations**\n",
    "   - TF-IDF vectorization\n",
    "   - Word embeddings\n",
    "   - Contextual embeddings\n",
    "\n",
    "3. **Transformer Architecture**\n",
    "   - Attention mechanisms\n",
    "   - Pre-trained models (BERT, DistilBERT)\n",
    "   - Fine-tuning for downstream tasks\n",
    "\n",
    "4. **NLP Applications**\n",
    "   - Text classification\n",
    "   - Sentiment analysis\n",
    "   - Document summarization\n",
    "   - Named entity recognition\n",
    "\n",
    "5. **Document Intelligence System**\n",
    "   - End-to-end NLP pipeline\n",
    "   - Multiple analysis capabilities\n",
    "   - Production-ready design\n",
    "\n",
    "### Engineering Best Practices\n",
    "\n",
    "- ✅ Preprocess text consistently\n",
    "- ✅ Use pre-trained models when possible\n",
    "- ✅ Fine-tune on domain-specific data\n",
    "- ✅ Evaluate on held-out test sets\n",
    "- ✅ Handle out-of-vocabulary words\n",
    "- ✅ Consider inference latency and cost\n",
    "- ✅ Monitor model performance over time\n",
    "\n",
    "### Next Week Preview\n",
    "\n",
    "**Week 7: Large Language Model Systems**\n",
    "- LLM architecture and capabilities\n",
    "- Prompt engineering patterns\n",
    "- LLM APIs and usage\n",
    "- Output evaluation\n",
    "- Building AI writing assistants\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Hugging Face Documentation**: https://huggingface.co/docs\n",
    "- **NLTK Book**: Natural Language Processing with Python\n",
    "- **Attention Is All You Need**: Original Transformer paper\n",
    "- **BERT Paper**: Pre-training of Deep Bidirectional Transformers\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Modern NLP is about leveraging pre-trained models and adapting them to your specific use case. Focus on data quality, evaluation, and production readiness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
