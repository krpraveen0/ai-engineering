{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Large Language Model Systems (Solution)\n\nComplete solutions for Week 7 exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import logging\n\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Client - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self, model: str = 'gpt-3.5-turbo', api_key: Optional[str] = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))\n",
    "    def complete(self, prompt: str, temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "        response = self.client.completions.create(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    \n",
    "    def chat_complete(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prompt Engineering - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    def __init__(self, template: str, variables: List[str]):\n",
    "        self.template = template\n",
    "        self.variables = variables\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        for var in self.variables:\n",
    "            if var not in kwargs:\n",
    "                raise ValueError(f'Missing variable: {var}')\n",
    "        return self.template.format(**kwargs)\n\n",
    "class PromptLibrary:\n",
    "    @staticmethod\n",
    "    def zero_shot(task: str, input_text: str) -> str:\n",
    "        return f'{task}\\n\\nInput: {input_text}\\n\\nOutput:'\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot(task: str, examples: List[Tuple[str, str]], input_text: str) -> str:\n",
    "        prompt = f'{task}\\n\\nExamples:\\n'\n",
    "        for inp, out in examples:\n",
    "            prompt += f'Input: {inp}\\nOutput: {out}\\n\\n'\n",
    "        prompt += f'Now your turn:\\nInput: {input_text}\\nOutput:'\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(problem: str) -> str:\n",
    "        return f'{problem}\\n\\nLet\\'s think step by step:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Output Evaluation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n\n",
    "class OutputEvaluator:\n",
    "    def check_relevance(self, output: str, query: str) -> float:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform([query, output])\n",
    "        return float(cosine_similarity(vectors[0:1], vectors[1:2])[0][0])\n",
    "    \n",
    "    def check_coherence(self, output: str) -> float:\n",
    "        sentences = output.split('.')\n",
    "        if len(sentences) < 2:\n",
    "            return 1.0\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            vectors = vectorizer.fit_transform(sentences)\n",
    "            similarities = []\n",
    "            for i in range(len(sentences)-1):\n",
    "                sim = cosine_similarity(vectors[i:i+1], vectors[i+1:i+2])[0][0]\n",
    "                similarities.append(sim)\n",
    "            return float(np.mean(similarities))\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    def check_factuality(self, output: str, reference: Optional[str] = None) -> float:\n",
    "        if reference is None:\n",
    "            return 0.5\n",
    "        return self.check_relevance(output, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: AI Writing Assistant - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIWritingAssistant:\n",
    "    def __init__(self, model: str = 'gpt-3.5-turbo'):\n",
    "        self.client = LLMClient(model)\n",
    "        self.evaluator = OutputEvaluator()\n",
    "    \n",
    "    def generate_content(self, topic: str, style: str = 'professional', length: int = 500) -> str:\n",
    "        prompt = f'Write a {style} {length}-word article about: {topic}'\n",
    "        return self.client.complete(prompt, max_tokens=length*2)\n",
    "    \n",
    "    def improve_text(self, text: str, aspect: str = 'clarity') -> str:\n",
    "        prompt = f'Improve the {aspect} of this text:\\n\\n{text}\\n\\nImproved version:'\n",
    "        return self.client.complete(prompt)\n",
    "    \n",
    "    def analyze_text(self, text: str) -> Dict:\n",
    "        prompt = f'Analyze this text and provide feedback on clarity, coherence, and style:\\n\\n{text}'\n",
    "        analysis = self.client.complete(prompt)\n",
    "        return {\n",
    "            'analysis': analysis,\n",
    "            'coherence_score': self.evaluator.check_coherence(text)\n",
    "        }\n",
    "    \n",
    "    def summarize(self, text: str, max_length: int = 100) -> str:\n",
    "        prompt = f'Summarize in {max_length} words:\\n\\n{text}'\n",
    "        return self.client.complete(prompt, max_tokens=max_length*2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}