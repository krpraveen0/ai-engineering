{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Deep Learning Essentials & Vision\n",
    "\n",
    "## Overview\n",
    "Welcome to Week 5 of the AI Engineering curriculum. This week marks your transition into **deep learning** and representation learning. You'll learn how neural networks automatically learn hierarchical features from data, with a focus on computer vision applications.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this week, you will be able to:\n",
    "- Understand neural network fundamentals and architecture\n",
    "- Grasp backpropagation intuition (conceptual, not mathematical)\n",
    "- Build and train Convolutional Neural Networks (CNNs) for image classification\n",
    "- Apply transfer learning using pre-trained models\n",
    "- Identify and mitigate overfitting using regularization techniques\n",
    "- Evaluate vision models properly\n",
    "\n",
    "### Real-World Outcome\n",
    "Build a **Visual Defect Detection System** that can identify manufacturing defects in products using deep learning.\n",
    "\n",
    "### Prerequisites\n",
    "- Python fundamentals (Week 1)\n",
    "- NumPy and data processing (Week 1)\n",
    "- Machine Learning basics (Week 4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Neural Network Fundamentals\n",
    "\n",
    "### 1.1 Understanding Neural Networks\n",
    "\n",
    "Neural networks are composed of layers of interconnected neurons that transform inputs through learned weights and biases.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Neuron**: Basic unit that computes weighted sum + bias, then applies activation\n",
    "- **Layer**: Collection of neurons processing in parallel\n",
    "- **Forward Pass**: Data flows from input → hidden layers → output\n",
    "- **Activation Functions**: Non-linear transformations (ReLU, Sigmoid, Tanh)\n",
    "- **Loss Function**: Measures prediction error\n",
    "- **Backpropagation**: Algorithm to compute gradients for weight updates\n",
    "\n",
    "### TODO 1.1: Implement a Simple Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralLayer:\n",
    "    \"\"\"\n",
    "    A simple neural network layer with forward pass.\n",
    "    Demonstrates the basic building block of neural networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Initialize layer with random weights and zero bias.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            output_size: Number of output neurons\n",
    "        \"\"\"\n",
    "        # TODO: Initialize weights with small random values (use np.random.randn)\n",
    "        # Shape should be (input_size, output_size)\n",
    "        self.weights = None  # Replace with proper initialization\n",
    "        \n",
    "        # TODO: Initialize bias as zeros\n",
    "        # Shape should be (output_size,)\n",
    "        self.bias = None  # Replace with proper initialization\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass: compute output = x @ weights + bias\n",
    "        \n",
    "        Args:\n",
    "            x: Input array of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output array of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: Use np.dot() or @ operator for matrix multiplication\n",
    "        pass\n",
    "    \n",
    "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: max(0, x)\n",
    "        \"\"\"\n",
    "        # TODO: Implement ReLU activation\n",
    "        pass\n",
    "\n",
    "# Test the layer\n",
    "# TODO: Uncomment and test\n",
    "# layer = SimpleNeuralLayer(10, 5)\n",
    "# test_input = np.random.randn(3, 10)  # 3 samples, 10 features\n",
    "# output = layer.forward(test_input)\n",
    "# print(f\"Input shape: {test_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Output with ReLU shape: {layer.relu(output).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building a Multi-Layer Network with PyTorch\n",
    "\n",
    "Now let's build a proper neural network using PyTorch, a production-grade deep learning framework.\n",
    "\n",
    "### TODO 1.2: Implement a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for classification.\n",
    "    A feedforward neural network with hidden layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize MLP architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        # TODO: Build the network architecture\n",
    "        # Create a sequential model with:\n",
    "        # 1. Input layer → first hidden layer\n",
    "        # 2. ReLU activation\n",
    "        # 3. Dropout\n",
    "        # 4. Additional hidden layers (iterate through hidden_sizes)\n",
    "        # 5. Output layer\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # TODO: Add input to first hidden layer\n",
    "        # layers.append(nn.Linear(?, ?))\n",
    "        # layers.append(nn.ReLU())\n",
    "        # layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # TODO: Add remaining hidden layers\n",
    "        # for i in range(len(hidden_sizes) - 1):\n",
    "        #     ...\n",
    "        \n",
    "        # TODO: Add output layer\n",
    "        # layers.append(nn.Linear(?, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "        \n",
    "        Returns:\n",
    "            Output logits\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through the network\n",
    "        return self.network(x)\n",
    "\n",
    "# Test the MLP\n",
    "# TODO: Uncomment and test\n",
    "# model = MLPClassifier(input_size=784, hidden_sizes=[256, 128], num_classes=10)\n",
    "# print(model)\n",
    "# test_input = torch.randn(32, 784)  # Batch of 32, each with 784 features\n",
    "# output = model(test_input)\n",
    "# print(f\"Output shape: {output.shape}\")  # Should be (32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "### 2.1 Understanding CNNs\n",
    "\n",
    "CNNs are specialized for processing grid-like data (images). They use:\n",
    "- **Convolutional Layers**: Learn spatial hierarchies of features\n",
    "- **Pooling Layers**: Reduce spatial dimensions\n",
    "- **Fully Connected Layers**: Final classification\n",
    "\n",
    "**Why CNNs for Vision?**\n",
    "- **Parameter Sharing**: Same filter applied across image\n",
    "- **Spatial Hierarchy**: Low-level → high-level features\n",
    "- **Translation Invariance**: Detect features regardless of position\n",
    "\n",
    "### TODO 2.1: Implement a Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic CNN for image classification.\n",
    "    Architecture: Conv → ReLU → Pool → Conv → ReLU → Pool → FC → FC\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10, input_channels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize CNN architecture.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "            input_channels: Number of input channels (3 for RGB, 1 for grayscale)\n",
    "        \"\"\"\n",
    "        super(BasicCNN, self).__init__()\n",
    "        \n",
    "        # TODO: Define convolutional layers\n",
    "        # Layer 1: Conv(input_channels, 32, kernel=3, padding=1) → ReLU → MaxPool(2)\n",
    "        self.conv1 = None  # Replace with nn.Conv2d\n",
    "        self.pool1 = None  # Replace with nn.MaxPool2d\n",
    "        \n",
    "        # Layer 2: Conv(32, 64, kernel=3, padding=1) → ReLU → MaxPool(2)\n",
    "        self.conv2 = None  # Replace with nn.Conv2d\n",
    "        self.pool2 = None  # Replace with nn.MaxPool2d\n",
    "        \n",
    "        # Layer 3: Conv(64, 128, kernel=3, padding=1) → ReLU → MaxPool(2)\n",
    "        self.conv3 = None  # Replace with nn.Conv2d\n",
    "        self.pool3 = None  # Replace with nn.MaxPool2d\n",
    "        \n",
    "        # TODO: Define fully connected layers\n",
    "        # Calculate flattened size based on input image size\n",
    "        # For 32x32 input: after 3 pooling layers → 4x4 spatial size\n",
    "        # So: 128 channels × 4 × 4 = 2048\n",
    "        self.fc1 = None  # Replace with nn.Linear\n",
    "        self.fc2 = None  # Replace with nn.Linear\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through CNN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # 1. Pass through conv1 → ReLU → pool1\n",
    "        # x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        \n",
    "        # 2. Pass through conv2 → ReLU → pool2\n",
    "        # ...\n",
    "        \n",
    "        # 3. Pass through conv3 → ReLU → pool3\n",
    "        # ...\n",
    "        \n",
    "        # 4. Flatten: x = x.view(x.size(0), -1)\n",
    "        # ...\n",
    "        \n",
    "        # 5. Pass through fc1 → ReLU → dropout → fc2\n",
    "        # ...\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test the CNN\n",
    "# TODO: Uncomment and test\n",
    "# model = BasicCNN(num_classes=10, input_channels=3)\n",
    "# print(model)\n",
    "# test_input = torch.randn(8, 3, 32, 32)  # Batch of 8 RGB images 32x32\n",
    "# output = model(test_input)\n",
    "# print(f\"Output shape: {output.shape}\")  # Should be (8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Augmentation\n",
    "\n",
    "Data augmentation artificially increases dataset diversity by applying transformations to training images.\n",
    "\n",
    "### TODO 2.2: Implement Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset with augmentation support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            images: Array of images\n",
    "            labels: Array of labels\n",
    "            transform: Torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # TODO: Return dataset size\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get item at index.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (transformed_image, label)\n",
    "        \"\"\"\n",
    "        # TODO: Get image and label at index\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # TODO: Convert numpy array to PIL Image\n",
    "        # image = Image.fromarray(image)\n",
    "        \n",
    "        # TODO: Apply transformations if provided\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        \n",
    "        pass\n",
    "\n",
    "def get_data_transforms(train: bool = True):\n",
    "    \"\"\"\n",
    "    Get data augmentation transforms.\n",
    "    \n",
    "    Args:\n",
    "        train: If True, return training transforms with augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Composed transforms\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # TODO: Define training transforms with augmentation\n",
    "        # Include: RandomHorizontalFlip, RandomRotation, ColorJitter, ToTensor, Normalize\n",
    "        transform = transforms.Compose([\n",
    "            # Add transforms here\n",
    "        ])\n",
    "    else:\n",
    "        # TODO: Define validation/test transforms (no augmentation)\n",
    "        # Include: ToTensor, Normalize\n",
    "        transform = transforms.Compose([\n",
    "            # Add transforms here\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "# Example usage\n",
    "# TODO: Test with sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training Pipeline\n",
    "\n",
    "### 3.1 Training Loop\n",
    "\n",
    "A proper training loop includes:\n",
    "- Forward pass\n",
    "- Loss computation\n",
    "- Backward pass (gradient computation)\n",
    "- Parameter update\n",
    "- Metrics tracking\n",
    "\n",
    "### TODO 3.1: Implement Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for CNN models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, criterion, optimizer) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Train for one epoch.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (average_loss, accuracy)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            # TODO: Move data to device\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # TODO: Zero gradients\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            # TODO: Forward pass\n",
    "            # outputs = self.model(images)\n",
    "            \n",
    "            # TODO: Compute loss\n",
    "            # loss = criterion(outputs, labels)\n",
    "            \n",
    "            # TODO: Backward pass\n",
    "            # loss.backward()\n",
    "            \n",
    "            # TODO: Update weights\n",
    "            # optimizer.step()\n",
    "            \n",
    "            # TODO: Track metrics\n",
    "            # running_loss += loss.item()\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            # total += labels.size(0)\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        # TODO: Calculate average loss and accuracy\n",
    "        avg_loss = 0.0  # Replace\n",
    "        accuracy = 0.0  # Replace\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader: DataLoader, criterion) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Validate model.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (average_loss, accuracy)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # TODO: Implement validation loop (similar to training but without gradient updates)\n",
    "        # Use torch.no_grad() context\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                # TODO: Move to device, forward pass, compute loss and metrics\n",
    "                pass\n",
    "        \n",
    "        avg_loss = 0.0  # Replace\n",
    "        accuracy = 0.0  # Replace\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, \n",
    "              num_epochs: int, learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        Full training pipeline.\n",
    "        \"\"\"\n",
    "        # TODO: Define loss function (CrossEntropyLoss for classification)\n",
    "        criterion = None\n",
    "        \n",
    "        # TODO: Define optimizer (Adam is a good default)\n",
    "        optimizer = None\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # TODO: Train for one epoch\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)\n",
    "            \n",
    "            # TODO: Validate\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # TODO: Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Log progress\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                       f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "                       f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plot training and validation metrics.\n",
    "        \"\"\"\n",
    "        # TODO: Create plots for loss and accuracy\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Train Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Transfer Learning\n",
    "\n",
    "### 4.1 Understanding Transfer Learning\n",
    "\n",
    "Transfer learning leverages pre-trained models trained on large datasets (e.g., ImageNet) and adapts them to new tasks.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster training\n",
    "- Better performance with less data\n",
    "- Learned features are reusable\n",
    "\n",
    "**Approaches:**\n",
    "1. **Feature Extraction**: Freeze pre-trained layers, train only final classifier\n",
    "2. **Fine-tuning**: Unfreeze some layers and train with low learning rate\n",
    "\n",
    "### TODO 4.1: Implement Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningModel:\n",
    "    \"\"\"\n",
    "    Transfer learning wrapper using pre-trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_classes: int, freeze_layers: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize transfer learning model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of pre-trained model ('resnet18', 'resnet50', 'vgg16', etc.)\n",
    "            num_classes: Number of classes in target task\n",
    "            freeze_layers: If True, freeze pre-trained layers\n",
    "        \"\"\"\n",
    "        # TODO: Load pre-trained model\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            num_features = self.model.fc.in_features\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "            num_features = self.model.fc.in_features\n",
    "        elif model_name == 'vgg16':\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            num_features = self.model.classifier[6].in_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # TODO: Freeze pre-trained layers if specified\n",
    "        if freeze_layers:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # TODO: Replace final layer with custom classifier\n",
    "        if 'resnet' in model_name:\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        elif 'vgg' in model_name:\n",
    "            self.model.classifier[6] = nn.Sequential(\n",
    "                nn.Linear(num_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "    \n",
    "    def get_model(self) -> nn.Module:\n",
    "        \"\"\"Return the model.\"\"\"\n",
    "        return self.model\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers: int = None):\n",
    "        \"\"\"\n",
    "        Unfreeze layers for fine-tuning.\n",
    "        \n",
    "        Args:\n",
    "            num_layers: Number of layers to unfreeze from the end. If None, unfreeze all.\n",
    "        \"\"\"\n",
    "        # TODO: Implement layer unfreezing for fine-tuning\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "# TODO: Test transfer learning model\n",
    "# tl_model = TransferLearningModel('resnet18', num_classes=5, freeze_layers=True)\n",
    "# model = tl_model.get_model()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Regularization & Overfitting\n",
    "\n",
    "### 5.1 Understanding Overfitting\n",
    "\n",
    "**Overfitting** occurs when model learns training data too well, including noise, and fails to generalize.\n",
    "\n",
    "**Signs of Overfitting:**\n",
    "- Training accuracy high, validation accuracy low\n",
    "- Large gap between train and validation loss\n",
    "\n",
    "**Regularization Techniques:**\n",
    "1. **Dropout**: Randomly drop neurons during training\n",
    "2. **L2 Regularization (Weight Decay)**: Penalize large weights\n",
    "3. **Early Stopping**: Stop training when validation loss stops improving\n",
    "4. **Data Augmentation**: Increase effective dataset size\n",
    "5. **Batch Normalization**: Normalize layer inputs\n",
    "\n",
    "### TODO 5.1: Implement Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting.\n",
    "    Stops training when validation loss doesn't improve for patience epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.0, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait before stopping\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "            verbose: If True, print messages\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "        \n",
    "        Returns:\n",
    "            True if training should stop\n",
    "        \"\"\"\n",
    "        # TODO: Implement early stopping logic\n",
    "        # 1. If this is first call, set best_loss\n",
    "        # 2. If val_loss improved by at least min_delta, reset counter\n",
    "        # 3. Otherwise, increment counter\n",
    "        # 4. If counter >= patience, set early_stop = True\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "# early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "# for epoch in range(100):\n",
    "#     # ... training code ...\n",
    "#     val_loss = validate(...)\n",
    "#     if early_stopping(val_loss):\n",
    "#         print(\"Early stopping triggered\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visual Defect Detection System (Project)\n",
    "\n",
    "### 6.1 Problem Statement\n",
    "\n",
    "Build a system to automatically detect defects in manufactured products using computer vision.\n",
    "\n",
    "**Requirements:**\n",
    "- Binary classification: defective vs. non-defective\n",
    "- High recall (minimize false negatives)\n",
    "- Fast inference for production line\n",
    "- Explainable predictions\n",
    "\n",
    "### TODO 6.1: Build Complete Defect Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDetectionSystem:\n",
    "    \"\"\"\n",
    "    Complete system for visual defect detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'cnn', use_transfer_learning: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize defect detection system.\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'cnn' or 'transfer' for transfer learning\n",
    "            use_transfer_learning: If True, use pre-trained model\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "        # TODO: Initialize appropriate model\n",
    "        if use_transfer_learning:\n",
    "            # Use transfer learning\n",
    "            pass\n",
    "        else:\n",
    "            # Use custom CNN\n",
    "            pass\n",
    "    \n",
    "    def prepare_data(self, data_path: str, test_size: float = 0.2, val_size: float = 0.1):\n",
    "        \"\"\"\n",
    "        Load and prepare data.\n",
    "        \n",
    "        Returns:\n",
    "            Train, validation, and test data loaders\n",
    "        \"\"\"\n",
    "        # TODO: Implement data loading and splitting\n",
    "        pass\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs: int = 50):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        # TODO: Initialize trainer and train model\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, test_loader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        # TODO: Compute and return metrics\n",
    "        # Include: accuracy, precision, recall, F1-score\n",
    "        pass\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Predict on single image.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (prediction, confidence)\n",
    "        \"\"\"\n",
    "        # TODO: Implement inference\n",
    "        pass\n",
    "    \n",
    "    def visualize_predictions(self, images: np.ndarray, labels: np.ndarray, num_samples: int = 8):\n",
    "        \"\"\"\n",
    "        Visualize predictions on sample images.\n",
    "        \"\"\"\n",
    "        # TODO: Create visualization of predictions\n",
    "        pass\n",
    "\n",
    "# TODO: Build and test the complete system\n",
    "# system = DefectDetectionSystem(use_transfer_learning=True)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Model Evaluation & Interpretation\n",
    "\n",
    "### 7.1 Evaluation Metrics for Vision\n",
    "\n",
    "### TODO 7.1: Implement Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation utilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute comprehensive metrics.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate metrics using sklearn\n",
    "        # Include: accuracy, precision, recall, F1, confusion matrix\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str]):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix.\n",
    "        \"\"\"\n",
    "        # TODO: Create heatmap of confusion matrix\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_misclassifications(images: np.ndarray, y_true: np.ndarray, \n",
    "                                     y_pred: np.ndarray, class_names: List[str], num_samples: int = 9):\n",
    "        \"\"\"\n",
    "        Visualize misclassified samples.\n",
    "        \"\"\"\n",
    "        # TODO: Show images that were misclassified\n",
    "        pass\n",
    "\n",
    "# TODO: Test evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Best Practices & Production Considerations\n",
    "\n",
    "### 8.1 Model Saving and Loading\n",
    "\n",
    "### TODO 8.1: Implement Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Save and load model checkpoints.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(model: nn.Module, path: str, metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        Save model with metadata.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            path: Save path\n",
    "            metadata: Optional metadata (training config, metrics, etc.)\n",
    "        \"\"\"\n",
    "        # TODO: Save model state dict and metadata\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(model: nn.Module, path: str) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Load model from checkpoint.\n",
    "        \"\"\"\n",
    "        # TODO: Load model state dict\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "# ModelCheckpoint.save_model(model, 'defect_detector.pth', metadata={'accuracy': 0.95})\n",
    "# loaded_model = ModelCheckpoint.load_model(BasicCNN(num_classes=2), 'defect_detector.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### What You Learned This Week\n",
    "\n",
    "1. **Neural Network Fundamentals**\n",
    "   - Architecture components: layers, neurons, activations\n",
    "   - Forward and backward propagation\n",
    "   - Loss functions and optimization\n",
    "\n",
    "2. **Convolutional Neural Networks**\n",
    "   - Convolution and pooling operations\n",
    "   - Hierarchical feature learning\n",
    "   - CNN architectures for vision\n",
    "\n",
    "3. **Transfer Learning**\n",
    "   - Leveraging pre-trained models\n",
    "   - Feature extraction vs. fine-tuning\n",
    "   - Practical benefits and tradeoffs\n",
    "\n",
    "4. **Regularization**\n",
    "   - Dropout, weight decay, data augmentation\n",
    "   - Early stopping\n",
    "   - Preventing overfitting\n",
    "\n",
    "5. **Production System**\n",
    "   - Complete defect detection pipeline\n",
    "   - Model evaluation and interpretation\n",
    "   - Model persistence\n",
    "\n",
    "### Engineering Best Practices\n",
    "\n",
    "- ✅ Use data augmentation to improve generalization\n",
    "- ✅ Monitor both training and validation metrics\n",
    "- ✅ Start with transfer learning before training from scratch\n",
    "- ✅ Implement early stopping to prevent overfitting\n",
    "- ✅ Save model checkpoints during training\n",
    "- ✅ Evaluate on held-out test set\n",
    "- ✅ Consider computational constraints for production\n",
    "\n",
    "### Next Week Preview\n",
    "\n",
    "**Week 6: NLP & Transformers**\n",
    "- Text preprocessing and tokenization\n",
    "- Word embeddings and semantic representations\n",
    "- Transformer architecture\n",
    "- Building document intelligence systems\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **PyTorch Documentation**: https://pytorch.org/docs/\n",
    "- **Deep Learning Book (Goodfellow et al.)**: Chapter 6-9\n",
    "- **CS231n Course**: Stanford's Convolutional Neural Networks course\n",
    "- **Transfer Learning Guide**: PyTorch transfer learning tutorial\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Deep learning is about building systems that learn representations, not just memorize patterns. Always think about generalization and real-world deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
