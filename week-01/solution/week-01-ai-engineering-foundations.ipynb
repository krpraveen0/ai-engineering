{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: AI Engineering Mindset & Python Foundations - SOLUTION\n",
    "\n",
    "## Overview\n",
    "This solution notebook provides complete implementations for all Week 1 exercises.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this week, you will be able to:\n",
    "- Distinguish between AI, ML, DL, and Agentic AI\n",
    "- Understand the AI system lifecycle: data → model → system → production\n",
    "- Write clean, production-quality Python code with OOP, modularity, and typing\n",
    "- Apply NumPy for efficient numerical computations\n",
    "- Process data using Pandas\n",
    "- Implement data validation and logging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: AI vs ML vs DL vs Agentic AI\n",
    "\n",
    "### Solution 1.1: Map Use Cases to AI Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from enum import Enum\n",
    "\n",
    "class AICategory(Enum):\n",
    "    RULE_BASED_AI = \"rule_based_ai\"\n",
    "    TRADITIONAL_ML = \"traditional_ml\"\n",
    "    DEEP_LEARNING = \"deep_learning\"\n",
    "    AGENTIC_AI = \"agentic_ai\"\n",
    "\n",
    "def categorize_ai_system(description: str) -> AICategory:\n",
    "    \"\"\"\n",
    "    Categorize an AI system based on its description.\n",
    "    \n",
    "    Args:\n",
    "        description: Description of the AI system\n",
    "    \n",
    "    Returns:\n",
    "        AICategory enum value\n",
    "    \"\"\"\n",
    "    description_lower = description.lower()\n",
    "    \n",
    "    # Check for agentic AI keywords\n",
    "    if any(keyword in description_lower for keyword in ['autonomous', 'agent', 'plan', 'tool', 'memory']):\n",
    "        return AICategory.AGENTIC_AI\n",
    "    \n",
    "    # Check for deep learning keywords\n",
    "    if any(keyword in description_lower for keyword in ['neural network', 'cnn', 'transformer', 'deep learning', 'convolution']):\n",
    "        return AICategory.DEEP_LEARNING\n",
    "    \n",
    "    # Check for rule-based keywords\n",
    "    if any(keyword in description_lower for keyword in ['rule', 'if-then', 'expert system', 'logic']):\n",
    "        return AICategory.RULE_BASED_AI\n",
    "    \n",
    "    # Default to traditional ML\n",
    "    return AICategory.TRADITIONAL_ML\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"A spam filter using logistic regression\",\n",
    "    \"A chess program with if-then rules\",\n",
    "    \"An image classifier using convolutional neural networks\",\n",
    "    \"An autonomous agent that plans and executes research tasks\"\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    print(f\"{case}: {categorize_ai_system(case)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: AI System Lifecycle\n",
    "\n",
    "### Solution 2.1: Design a System Lifecycle Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class LifecycleStage:\n",
    "    \"\"\"Represents a stage in the AI system lifecycle.\"\"\"\n",
    "    name: str\n",
    "    started_at: Optional[datetime] = None\n",
    "    completed_at: Optional[datetime] = None\n",
    "    status: str = \"pending\"  # pending, in_progress, completed, failed\n",
    "    \n",
    "class AISystemLifecycle:\n",
    "    \"\"\"Tracks the lifecycle of an AI system.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.stages = {\n",
    "            \"data\": LifecycleStage(\"Data Collection & Preparation\"),\n",
    "            \"model\": LifecycleStage(\"Model Development\"),\n",
    "            \"system\": LifecycleStage(\"System Integration\"),\n",
    "            \"production\": LifecycleStage(\"Production Deployment\")\n",
    "        }\n",
    "    \n",
    "    def start_stage(self, stage_name: str) -> None:\n",
    "        \"\"\"Mark a stage as started.\"\"\"\n",
    "        if stage_name not in self.stages:\n",
    "            raise ValueError(f\"Invalid stage: {stage_name}\")\n",
    "        \n",
    "        stage = self.stages[stage_name]\n",
    "        stage.started_at = datetime.now()\n",
    "        stage.status = \"in_progress\"\n",
    "        print(f\"Started stage: {stage.name}\")\n",
    "    \n",
    "    def complete_stage(self, stage_name: str) -> None:\n",
    "        \"\"\"Mark a stage as completed.\"\"\"\n",
    "        if stage_name not in self.stages:\n",
    "            raise ValueError(f\"Invalid stage: {stage_name}\")\n",
    "        \n",
    "        stage = self.stages[stage_name]\n",
    "        if stage.status != \"in_progress\":\n",
    "            print(f\"Warning: Stage {stage.name} was not in progress\")\n",
    "        \n",
    "        stage.completed_at = datetime.now()\n",
    "        stage.status = \"completed\"\n",
    "        print(f\"Completed stage: {stage.name}\")\n",
    "    \n",
    "    def get_current_stage(self) -> Optional[str]:\n",
    "        \"\"\"Get the current in-progress stage.\"\"\"\n",
    "        for name, stage in self.stages.items():\n",
    "            if stage.status == \"in_progress\":\n",
    "                return name\n",
    "        return None\n",
    "    \n",
    "    def get_progress_report(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate a progress report of all stages.\"\"\"\n",
    "        return {name: stage.status for name, stage in self.stages.items()}\n",
    "\n",
    "# Test the lifecycle tracker\n",
    "lifecycle = AISystemLifecycle(\"Customer Churn Predictor\")\n",
    "print(f\"Project: {lifecycle.project_name}\")\n",
    "print(f\"Initial progress: {lifecycle.get_progress_report()}\")\n",
    "\n",
    "lifecycle.start_stage(\"data\")\n",
    "print(f\"Current stage: {lifecycle.get_current_stage()}\")\n",
    "lifecycle.complete_stage(\"data\")\n",
    "\n",
    "lifecycle.start_stage(\"model\")\n",
    "print(f\"Progress: {lifecycle.get_progress_report()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Production-Quality Python\n",
    "\n",
    "### Solution 3.1: Build a Data Validator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Tuple\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ValidationRule:\n",
    "    \"\"\"Represents a single validation rule.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, validator: Callable[[Any], bool], error_message: str):\n",
    "        self.name = name\n",
    "        self.validator = validator\n",
    "        self.error_message = error_message\n",
    "    \n",
    "    def validate(self, value: Any) -> Tuple[bool, str]:\n",
    "        \"\"\"Run validation and return (is_valid, message).\"\"\"\n",
    "        try:\n",
    "            is_valid = self.validator(value)\n",
    "            if is_valid:\n",
    "                return (True, \"\")\n",
    "            else:\n",
    "                return (False, self.error_message)\n",
    "        except Exception as e:\n",
    "            return (False, f\"Validation error: {str(e)}\")\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validates data against a set of rules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: List[ValidationRule] = []\n",
    "    \n",
    "    def add_rule(self, rule: ValidationRule) -> None:\n",
    "        \"\"\"Add a validation rule.\"\"\"\n",
    "        self.rules.append(rule)\n",
    "        logger.info(f\"Added validation rule: {rule.name}\")\n",
    "    \n",
    "    def validate(self, value: Any) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate value against all rules.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_valid, list of error messages)\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for rule in self.rules:\n",
    "            is_valid, message = rule.validate(value)\n",
    "            if not is_valid:\n",
    "                errors.append(f\"{rule.name}: {message}\")\n",
    "                logger.warning(f\"Validation failed for rule '{rule.name}': {message}\")\n",
    "        \n",
    "        return (len(errors) == 0, errors)\n",
    "\n",
    "# Create validation rules for email addresses\n",
    "email_validator = DataValidator()\n",
    "\n",
    "# Rule 1: Must contain @\n",
    "email_validator.add_rule(\n",
    "    ValidationRule(\n",
    "        \"contains_at\",\n",
    "        lambda email: '@' in str(email),\n",
    "        \"Email must contain @ symbol\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule 2: Must have domain\n",
    "email_validator.add_rule(\n",
    "    ValidationRule(\n",
    "        \"has_domain\",\n",
    "        lambda email: '.' in str(email).split('@')[-1] if '@' in str(email) else False,\n",
    "        \"Email must have a valid domain\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule 3: Must not be empty\n",
    "email_validator.add_rule(\n",
    "    ValidationRule(\n",
    "        \"not_empty\",\n",
    "        lambda email: len(str(email).strip()) > 0,\n",
    "        \"Email cannot be empty\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule 4: Basic format check\n",
    "email_validator.add_rule(\n",
    "    ValidationRule(\n",
    "        \"valid_format\",\n",
    "        lambda email: re.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', str(email)) is not None,\n",
    "        \"Email format is invalid\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test with valid and invalid emails\n",
    "test_emails = [\n",
    "    \"user@example.com\",\n",
    "    \"invalid.email\",\n",
    "    \"@nodomain.com\",\n",
    "    \"user@domain\",\n",
    "    \"\",\n",
    "    \"valid.user@company.org\"\n",
    "]\n",
    "\n",
    "print(\"\\nEmail Validation Results:\")\n",
    "print(\"=\" * 60)\n",
    "for email in test_emails:\n",
    "    is_valid, errors = email_validator.validate(email)\n",
    "    status = \"✓ VALID\" if is_valid else \"✗ INVALID\"\n",
    "    print(f\"{status}: '{email}'\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: NumPy Fundamentals & Vectorization\n",
    "\n",
    "### Solution 4.1: Vectorize Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(1000000)\n",
    "\n",
    "def normalize_loop(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize array to range [0, 1] using a loop.\n",
    "    Formula: (x - min) / (max - min)\n",
    "    \"\"\"\n",
    "    min_val = arr.min()\n",
    "    max_val = arr.max()\n",
    "    result = np.zeros_like(arr)\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        result[i] = (arr[i] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def normalize_vectorized(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize array to range [0, 1] using vectorized operations.\n",
    "    \"\"\"\n",
    "    min_val = arr.min()\n",
    "    max_val = arr.max()\n",
    "    return (arr - min_val) / (max_val - min_val)\n",
    "\n",
    "# Compare performance\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with smaller array for loop version (to avoid long wait)\n",
    "small_data = data[:10000]\n",
    "\n",
    "# Loop version\n",
    "start = time.time()\n",
    "result_loop = normalize_loop(small_data)\n",
    "time_loop = time.time() - start\n",
    "print(f\"Loop version (10k elements): {time_loop:.4f} seconds\")\n",
    "\n",
    "# Vectorized version (small)\n",
    "start = time.time()\n",
    "result_vec_small = normalize_vectorized(small_data)\n",
    "time_vec_small = time.time() - start\n",
    "print(f\"Vectorized version (10k elements): {time_vec_small:.6f} seconds\")\n",
    "print(f\"Speedup (small): {time_loop / time_vec_small:.1f}x faster\")\n",
    "\n",
    "# Vectorized version (full)\n",
    "start = time.time()\n",
    "result_vec = normalize_vectorized(data)\n",
    "time_vec = time.time() - start\n",
    "print(f\"\\nVectorized version (1M elements): {time_vec:.6f} seconds\")\n",
    "\n",
    "# Verify results are the same\n",
    "assert np.allclose(result_loop, result_vec_small), \"Results don't match!\"\n",
    "print(\"\\n✓ Results verified: Loop and vectorized versions produce same output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4.2: Statistical Analysis with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def compute_statistics(arr: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistics for an array.\n",
    "    \n",
    "    Returns dict with: mean, median, std, min, max, q25, q75\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'mean': float(np.mean(arr)),\n",
    "        'median': float(np.median(arr)),\n",
    "        'std': float(np.std(arr)),\n",
    "        'min': float(np.min(arr)),\n",
    "        'max': float(np.max(arr)),\n",
    "        'q25': float(np.percentile(arr, 25)),\n",
    "        'q75': float(np.percentile(arr, 75)),\n",
    "        'variance': float(np.var(arr)),\n",
    "        'range': float(np.ptp(arr))  # peak-to-peak (max - min)\n",
    "    }\n",
    "\n",
    "# Test with sample data\n",
    "test_data = np.random.randn(1000)\n",
    "stats = compute_statistics(test_data)\n",
    "\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:12s}: {value:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Pandas for Data Processing\n",
    "\n",
    "### Solution 5.1: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'user_id': range(1, 101),\n",
    "    'age': np.random.randint(18, 70, 100),\n",
    "    'income': np.random.randint(20000, 150000, 100),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=100),\n",
    "    'is_active': np.random.choice([True, False], 100),\n",
    "    'total_purchases': np.random.randint(0, 50, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some missing values\n",
    "df.loc[np.random.choice(df.index, 10, replace=False), 'income'] = np.nan\n",
    "\n",
    "def explore_dataframe(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive exploration report for a DataFrame.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - shape: (rows, columns)\n",
    "    - dtypes: dict of column types\n",
    "    - missing: dict of missing value counts per column\n",
    "    - numeric_summary: summary stats for numeric columns\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'missing': df.isnull().sum().to_dict(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "        'numeric_summary': df.describe().to_dict(),\n",
    "        'memory_usage': df.memory_usage(deep=True).sum(),\n",
    "        'duplicate_rows': df.duplicated().sum()\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "report = explore_dataframe(df)\n",
    "\n",
    "print(\"DataFrame Exploration Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {report['shape']}\")\n",
    "print(f\"\\nData Types:\")\n",
    "for col, dtype in report['dtypes'].items():\n",
    "    print(f\"  {col:20s}: {dtype}\")\n",
    "\n",
    "print(f\"\\nMissing Values:\")\n",
    "for col, count in report['missing'].items():\n",
    "    pct = report['missing_pct'][col]\n",
    "    if count > 0:\n",
    "        print(f\"  {col:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMemory Usage: {report['memory_usage'] / 1024:.2f} KB\")\n",
    "print(f\"Duplicate Rows: {report['duplicate_rows']}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5.2: Clean and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Handles data cleaning operations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_values(df: pd.DataFrame, strategy: str = 'mean') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values in DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            strategy: 'mean', 'median', 'mode', or 'drop'\n",
    "        \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            df_clean = df_clean.dropna()\n",
    "        else:\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if df_clean[col].isnull().any():\n",
    "                    if strategy == 'mean':\n",
    "                        df_clean[col].fillna(df_clean[col].mean(), inplace=True)\n",
    "                    elif strategy == 'median':\n",
    "                        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "                    elif strategy == 'mode':\n",
    "                        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_outliers(df: pd.DataFrame, column: str, n_std: float = 3.0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove outliers from a specific column using standard deviation method.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column: Column name to check for outliers\n",
    "            n_std: Number of standard deviations for outlier threshold\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with outliers removed\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        mean = df_clean[column].mean()\n",
    "        std = df_clean[column].std()\n",
    "        \n",
    "        lower_bound = mean - n_std * std\n",
    "        upper_bound = mean + n_std * std\n",
    "        \n",
    "        outliers_mask = (df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)\n",
    "        outliers_removed = len(df_clean) - outliers_mask.sum()\n",
    "        \n",
    "        print(f\"Removed {outliers_removed} outliers from '{column}' column\")\n",
    "        \n",
    "        return df_clean[outliers_mask]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create derived features from existing columns.\n",
    "        \"\"\"\n",
    "        df_featured = df.copy()\n",
    "        \n",
    "        # Calculate days since signup\n",
    "        df_featured['days_since_signup'] = (pd.Timestamp.now() - df_featured['signup_date']).dt.days\n",
    "        \n",
    "        # Purchase frequency (avoid division by zero)\n",
    "        df_featured['purchase_frequency'] = df_featured['total_purchases'] / (df_featured['days_since_signup'] + 1)\n",
    "        \n",
    "        # Income bracket\n",
    "        df_featured['income_bracket'] = pd.cut(\n",
    "            df_featured['income'],\n",
    "            bins=[0, 40000, 80000, float('inf')],\n",
    "            labels=['low', 'medium', 'high']\n",
    "        )\n",
    "        \n",
    "        # Age group\n",
    "        df_featured['age_group'] = pd.cut(\n",
    "            df_featured['age'],\n",
    "            bins=[0, 30, 45, 60, float('inf')],\n",
    "            labels=['18-30', '31-45', '46-60', '60+']\n",
    "        )\n",
    "        \n",
    "        # Customer value score (simple example)\n",
    "        df_featured['customer_value_score'] = (\n",
    "            df_featured['total_purchases'] * 10 +\n",
    "            df_featured['is_active'].astype(int) * 50\n",
    "        )\n",
    "        \n",
    "        return df_featured\n",
    "\n",
    "# Test the data cleaner\n",
    "print(\"Original DataFrame:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Missing values in income: {df['income'].isnull().sum()}\")\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "df_cleaned = cleaner.handle_missing_values(df, strategy='median')\n",
    "print(f\"\\nAfter handling missing values:\")\n",
    "print(f\"Missing values in income: {df_cleaned['income'].isnull().sum()}\")\n",
    "\n",
    "df_featured = cleaner.create_features(df_cleaned)\n",
    "print(f\"\\nAfter feature engineering:\")\n",
    "print(f\"New columns: {[col for col in df_featured.columns if col not in df.columns]}\")\n",
    "print(\"\\nSample of enhanced data:\")\n",
    "print(df_featured[['user_id', 'age', 'age_group', 'income', 'income_bracket', 'purchase_frequency']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Data Validation & Logging\n",
    "\n",
    "### Solution 6.1: Implement Pipeline Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List\n",
    "import logging\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Validates data quality in a pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.validation_results = []\n",
    "    \n",
    "    def check_missing_values(self, df: pd.DataFrame, max_missing_pct: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Check if missing values are within acceptable threshold.\n",
    "        \"\"\"\n",
    "        missing_pct = df.isnull().sum() / len(df)\n",
    "        failed_columns = missing_pct[missing_pct > max_missing_pct]\n",
    "        \n",
    "        if len(failed_columns) > 0:\n",
    "            self.logger.error(f\"Missing value threshold exceeded in columns: {failed_columns.to_dict()}\")\n",
    "            self.validation_results.append({\n",
    "                'check': 'missing_values',\n",
    "                'passed': False,\n",
    "                'details': failed_columns.to_dict()\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(\"Missing values check passed\")\n",
    "        self.validation_results.append({\n",
    "            'check': 'missing_values',\n",
    "            'passed': True,\n",
    "            'details': 'All columns within threshold'\n",
    "        })\n",
    "        return True\n",
    "    \n",
    "    def check_schema(self, df: pd.DataFrame, expected_columns: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if DataFrame has expected columns.\n",
    "        \"\"\"\n",
    "        actual_columns = set(df.columns)\n",
    "        expected_columns_set = set(expected_columns)\n",
    "        \n",
    "        missing_columns = expected_columns_set - actual_columns\n",
    "        extra_columns = actual_columns - expected_columns_set\n",
    "        \n",
    "        if missing_columns or extra_columns:\n",
    "            self.logger.error(f\"Schema mismatch - Missing: {missing_columns}, Extra: {extra_columns}\")\n",
    "            self.validation_results.append({\n",
    "                'check': 'schema',\n",
    "                'passed': False,\n",
    "                'details': {'missing': list(missing_columns), 'extra': list(extra_columns)}\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(\"Schema check passed\")\n",
    "        self.validation_results.append({\n",
    "            'check': 'schema',\n",
    "            'passed': True,\n",
    "            'details': 'All expected columns present'\n",
    "        })\n",
    "        return True\n",
    "    \n",
    "    def check_data_types(self, df: pd.DataFrame, expected_types: Dict[str, str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if columns have expected data types.\n",
    "        \"\"\"\n",
    "        type_mismatches = {}\n",
    "        \n",
    "        for col, expected_type in expected_types.items():\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            actual_type = str(df[col].dtype)\n",
    "            if expected_type not in actual_type:\n",
    "                type_mismatches[col] = {'expected': expected_type, 'actual': actual_type}\n",
    "        \n",
    "        if type_mismatches:\n",
    "            self.logger.error(f\"Data type mismatches: {type_mismatches}\")\n",
    "            self.validation_results.append({\n",
    "                'check': 'data_types',\n",
    "                'passed': False,\n",
    "                'details': type_mismatches\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(\"Data type check passed\")\n",
    "        self.validation_results.append({\n",
    "            'check': 'data_types',\n",
    "            'passed': True,\n",
    "            'details': 'All data types match'\n",
    "        })\n",
    "        return True\n",
    "    \n",
    "    def check_value_ranges(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:\n",
    "        \"\"\"\n",
    "        Check if values in a column are within expected range.\n",
    "        \"\"\"\n",
    "        if column not in df.columns:\n",
    "            self.logger.error(f\"Column '{column}' not found\")\n",
    "            return False\n",
    "        \n",
    "        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            self.logger.error(f\"{len(out_of_range)} values out of range [{min_val}, {max_val}] in '{column}'\")\n",
    "            self.validation_results.append({\n",
    "                'check': f'value_range_{column}',\n",
    "                'passed': False,\n",
    "                'details': f'{len(out_of_range)} values out of range'\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(f\"Value range check passed for '{column}'\")\n",
    "        self.validation_results.append({\n",
    "            'check': f'value_range_{column}',\n",
    "            'passed': True,\n",
    "            'details': 'All values within range'\n",
    "        })\n",
    "        return True\n",
    "    \n",
    "    def get_validation_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive validation report.\n",
    "        \"\"\"\n",
    "        total_checks = len(self.validation_results)\n",
    "        passed_checks = sum(1 for r in self.validation_results if r['passed'])\n",
    "        \n",
    "        return {\n",
    "            'total_checks': total_checks,\n",
    "            'passed': passed_checks,\n",
    "            'failed': total_checks - passed_checks,\n",
    "            'success_rate': (passed_checks / total_checks * 100) if total_checks > 0 else 0,\n",
    "            'results': self.validation_results\n",
    "        }\n",
    "\n",
    "# Test the quality checker\n",
    "print(\"Data Quality Validation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qc = DataQualityChecker()\n",
    "\n",
    "# Run various checks\n",
    "qc.check_schema(df_featured, ['user_id', 'age', 'income', 'signup_date'])\n",
    "qc.check_data_types(df_featured, {'user_id': 'int', 'age': 'int', 'income': 'float'})\n",
    "qc.check_value_ranges(df_featured, 'age', 18, 70)\n",
    "qc.check_missing_values(df_featured, max_missing_pct=0.15)\n",
    "\n",
    "# Get report\n",
    "report = qc.get_validation_report()\n",
    "print(f\"\\nValidation Report:\")\n",
    "print(f\"Total Checks: {report['total_checks']}\")\n",
    "print(f\"Passed: {report['passed']}\")\n",
    "print(f\"Failed: {report['failed']}\")\n",
    "print(f\"Success Rate: {report['success_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for result in report['results']:\n",
    "    status = \"✓\" if result['passed'] else \"✗\"\n",
    "    print(f\"{status} {result['check']}: {result['details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Week 1 Project - Production Data Pipeline\n",
    "\n",
    "### Solution 7.1: Implement Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class ProductionDataPipeline:\n",
    "    \"\"\"\n",
    "    A production-grade data pipeline with validation, logging, and metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name: str):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.logger = self._setup_logging()\n",
    "        self.cleaner = DataCleaner()\n",
    "        self.quality_checker = DataQualityChecker()\n",
    "        self.metrics = {}\n",
    "        self.start_time = None\n",
    "    \n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Set up pipeline logging.\"\"\"\n",
    "        logger = logging.getLogger(f\"Pipeline.{self.pipeline_name}\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    def load_data(self, data_source: Any) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from source.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Loading data from source\")\n",
    "            \n",
    "            # Handle different source types\n",
    "            if isinstance(data_source, pd.DataFrame):\n",
    "                df = data_source.copy()\n",
    "            elif isinstance(data_source, str):\n",
    "                # Assume it's a file path\n",
    "                if data_source.endswith('.csv'):\n",
    "                    df = pd.read_csv(data_source)\n",
    "                elif data_source.endswith('.json'):\n",
    "                    df = pd.read_json(data_source)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file format: {data_source}\")\n",
    "            else:\n",
    "                raise ValueError(\"Data source must be DataFrame or file path\")\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def validate_raw_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate raw data before processing.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Validating raw data\")\n",
    "        \n",
    "        # Check for empty dataframe\n",
    "        if len(df) == 0:\n",
    "            self.logger.error(\"DataFrame is empty\")\n",
    "            return False\n",
    "        \n",
    "        # Check missing values\n",
    "        self.quality_checker.check_missing_values(df, max_missing_pct=0.5)\n",
    "        \n",
    "        # Get validation report\n",
    "        report = self.quality_checker.get_validation_report()\n",
    "        \n",
    "        if report['failed'] > 0:\n",
    "            self.logger.warning(f\"Validation had {report['failed']} failed checks\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and transform data.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Cleaning data\")\n",
    "        \n",
    "        try:\n",
    "            # Handle missing values\n",
    "            df_clean = self.cleaner.handle_missing_values(df, strategy='median')\n",
    "            self.logger.info(f\"Handled missing values\")\n",
    "            \n",
    "            # Create features (if applicable)\n",
    "            if all(col in df_clean.columns for col in ['signup_date', 'total_purchases']):\n",
    "                df_clean = self.cleaner.create_features(df_clean)\n",
    "                self.logger.info(\"Created derived features\")\n",
    "            \n",
    "            return df_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to clean data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def compute_metrics(self, df_raw: pd.DataFrame, df_clean: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute pipeline metrics.\n",
    "        \"\"\"\n",
    "        processing_time = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rows_processed = len(df_raw)\n",
    "        rows_removed = len(df_raw) - len(df_clean)\n",
    "        missing_before = df_raw.isnull().sum().sum()\n",
    "        missing_after = df_clean.isnull().sum().sum()\n",
    "        missing_handled = missing_before - missing_after\n",
    "        \n",
    "        # Data quality score (simple example)\n",
    "        completeness = 1 - (df_clean.isnull().sum().sum() / (len(df_clean) * len(df_clean.columns)))\n",
    "        quality_score = completeness * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'rows_processed': rows_processed,\n",
    "            'rows_in_output': len(df_clean),\n",
    "            'rows_removed': rows_removed,\n",
    "            'columns_input': len(df_raw.columns),\n",
    "            'columns_output': len(df_clean.columns),\n",
    "            'missing_values_handled': missing_handled,\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'data_quality_score': quality_score,\n",
    "            'throughput_rows_per_second': rows_processed / processing_time if processing_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        self.metrics = metrics\n",
    "        self.logger.info(f\"Computed metrics: quality_score={quality_score:.2f}%, processing_time={processing_time:.2f}s\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def export_data(self, df: pd.DataFrame, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Export cleaned data to file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Exporting data to {output_path}\")\n",
    "            \n",
    "            # Create directory if needed\n",
    "            Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Export based on file extension\n",
    "            if output_path.endswith('.csv'):\n",
    "                df.to_csv(output_path, index=False)\n",
    "            elif output_path.endswith('.json'):\n",
    "                df.to_json(output_path, orient='records', indent=2)\n",
    "            elif output_path.endswith('.parquet'):\n",
    "                df.to_parquet(output_path, index=False)\n",
    "            else:\n",
    "                # Default to CSV\n",
    "                df.to_csv(output_path, index=False)\n",
    "            \n",
    "            self.logger.info(f\"Successfully exported {len(df)} rows to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to export data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, data_source: Any, output_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the complete pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Pipeline execution report\n",
    "        \"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.logger.info(f\"Starting pipeline: {self.pipeline_name}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Load data\n",
    "            df_raw = self.load_data(data_source)\n",
    "            \n",
    "            # 2. Validate raw data\n",
    "            if not self.validate_raw_data(df_raw):\n",
    "                raise ValueError(\"Raw data validation failed\")\n",
    "            \n",
    "            # 3. Clean data\n",
    "            df_clean = self.clean_data(df_raw)\n",
    "            \n",
    "            # 4. Compute metrics\n",
    "            metrics = self.compute_metrics(df_raw, df_clean)\n",
    "            \n",
    "            # 5. Export data\n",
    "            self.export_data(df_clean, output_path)\n",
    "            \n",
    "            # 6. Create execution report\n",
    "            report = {\n",
    "                'pipeline_name': self.pipeline_name,\n",
    "                'status': 'success',\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'metrics': metrics,\n",
    "                'validation_report': self.quality_checker.get_validation_report(),\n",
    "                'output_path': output_path\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\"Pipeline completed successfully\")\n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            return {\n",
    "                'pipeline_name': self.pipeline_name,\n",
    "                'status': 'failed',\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Test the complete pipeline\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRODUCTION DATA PIPELINE EXECUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = ProductionDataPipeline(\"user_data_pipeline\")\n",
    "report = pipeline.run(df, \"/tmp/cleaned_user_data.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE EXECUTION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We've Built\n",
    "This solution demonstrates:\n",
    "- **Production-quality code** with proper OOP, typing, and error handling\n",
    "- **Efficient data processing** using NumPy vectorization and Pandas\n",
    "- **Comprehensive validation** with multiple quality checks\n",
    "- **Detailed logging** for debugging and monitoring\n",
    "- **Complete pipeline** that handles the full data lifecycle\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Engineering mindset**: Think systems, not scripts\n",
    "2. **Code quality**: Type hints, modularity, testability\n",
    "3. **Performance**: Vectorization is crucial for large datasets\n",
    "4. **Validation**: Always validate data at every stage\n",
    "5. **Observability**: Log everything important\n",
    "\n",
    "### Real-World Applications\n",
    "This pipeline pattern is used in:\n",
    "- ETL systems for data warehouses\n",
    "- ML feature engineering pipelines\n",
    "- Data quality monitoring\n",
    "- Automated reporting systems\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
