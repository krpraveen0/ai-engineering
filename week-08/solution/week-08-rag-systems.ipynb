{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Retrieval-Augmented Generation (Solution)\n\nComplete solutions for Week 8 RAG exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import logging\n\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Chunking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), self.chunk_size - self.overlap):\n",
    "            chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_sentence(self, text: str) -> List[str]:\n",
    "        import nltk\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        for sent in sentences:\n",
    "            sent_len = len(sent.split())\n",
    "            if current_length + sent_len > self.chunk_size and current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sent]\n",
    "                current_length = sent_len\n",
    "            else:\n",
    "                current_chunk.append(sent)\n",
    "                current_length += sent_len\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Store - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = 'documents'):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(collection_name)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.doc_id = 0\n",
    "    \n",
    "    def add_documents(self, documents: List[str], metadata: Optional[List[Dict]] = None):\n",
    "        embeddings = self.embedding_model.encode(documents).tolist()\n",
    "        ids = [f'doc_{self.doc_id + i}' for i in range(len(documents))]\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=documents,\n",
    "            metadatas=metadata or [{} for _ in documents],\n",
    "            ids=ids\n",
    "        )\n",
    "        self.doc_id += len(documents)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "        results = self.collection.query(query_embeddings=query_embedding, n_results=top_k)\n",
    "        docs = results['documents'][0]\n",
    "        distances = results['distances'][0]\n",
    "        return list(zip(docs, distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: RAG System - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, vector_store: VectorStore, llm_model: str = 'gpt-3.5-turbo'):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = llm_model\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        results = self.vector_store.search(query, top_k)\n",
    "        return [doc for doc, _ in results]\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        context_text = '\\n\\n'.join(context)\n",
    "        prompt = f'Context:\\n{context_text}\\n\\nQuestion: {query}\\n\\nAnswer based on the context:'\n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        context = self.retrieve(question)\n",
    "        answer = self.generate(question, context)\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context': context\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Enterprise Chatbot - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseKnowledgeChatbot:\n",
    "    def __init__(self):\n",
    "        self.chunker = DocumentChunker(chunk_size=300, overlap=50)\n",
    "        self.vector_store = VectorStore('enterprise_kb')\n",
    "        self.rag_system = RAGSystem(self.vector_store)\n",
    "    \n",
    "    def ingest_documents(self, documents: List[str]):\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.chunker.chunk_by_sentence(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vector_store.add_documents(all_chunks)\n",
    "        logger.info(f'Ingested {len(all_chunks)} chunks from {len(documents)} documents')\n",
    "    \n",
    "    def chat(self, question: str) -> str:\n",
    "        result = self.rag_system.query(question)\n",
    "        return result['answer']\n",
    "    \n",
    "    def evaluate_answer(self, answer: str, context: List[str]) -> Dict:\n",
    "        has_citation = any(ctx[:50] in answer for ctx in context)\n",
    "        return {\n",
    "            'answer_length': len(answer.split()),\n",
    "            'uses_context': has_citation,\n",
    "            'context_count': len(context)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chatbot\n",
    "chatbot = EnterpriseKnowledgeChatbot()\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    'Our company was founded in 2020. We specialize in AI solutions for healthcare.',\n",
    "    'Our vacation policy allows 20 days per year. Employees can roll over up to 5 unused days.',\n",
    "    'The office is located at 123 Main St. Working hours are 9 AM to 5 PM.'\n",
    "]\n",
    "\n",
    "# Ingest and query\n",
    "chatbot.ingest_documents(docs)\n",
    "answer = chatbot.chat('What is the vacation policy?')\n",
    "print(f'Answer: {answer}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}