{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Deployment, Monitoring & Reliability\n\n## Overview\nWelcome to Week 11 of the AI Engineering curriculum. This week focuses on **shipping AI systems to production** with proper deployment, monitoring, and reliability practices.\n\n### Learning Objectives\nBy the end of this week, you will be able to:\n- Build production APIs with FastAPI for AI services\n- Dockerize AI applications for consistent deployment\n- Deploy AI systems to cloud platforms\n- Implement comprehensive logging and monitoring\n- Detect and handle model drift\n- Ensure reliability and uptime\n\n### Real-World Outcome\nBuild and deploy a **Production AI Service** (ML or LLM-based) with monitoring and reliability features.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building APIs with FastAPI\n\n### Why FastAPI for AI Services?\n\n**Benefits:**\n- Fast performance (built on Starlette/Pydantic)\n- Automatic API documentation (Swagger/OpenAPI)\n- Type validation\n- Async support\n- Easy to test\n\n### API Design Principles\n1. **RESTful**: Standard HTTP methods\n2. **Versioning**: /v1/predict, /v2/predict\n3. **Error handling**: Proper status codes\n4. **Rate limiting**: Prevent abuse\n5. **Authentication**: Secure access\n\n### TODO 1.1: Build FastAPI Service for AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Request\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nimport time\nimport asyncio\n\napp = FastAPI(\n    title=\"AI Service API\",\n    description=\"Production AI service with ML/LLM capabilities\",\n    version=\"1.0.0\"\n)\n\n# Request/Response models\nclass PredictionRequest(BaseModel):\n    \"\"\"Request model for predictions.\"\"\"\n    input_data: Dict[str, Any]\n    model_version: Optional[str] = \"latest\"\n    return_probabilities: bool = False\n\nclass PredictionResponse(BaseModel):\n    \"\"\"Response model for predictions.\"\"\"\n    prediction: Any\n    confidence: Optional[float] = None\n    model_version: str\n    processing_time_ms: float\n\nclass HealthResponse(BaseModel):\n    \"\"\"Health check response.\"\"\"\n    status: str\n    model_loaded: bool\n    uptime_seconds: float\n\n# TODO: Implement AI model service\nclass AIModelService:\n    \"\"\"Service for managing AI model inference.\"\"\"\n    \n    def __init__(self, model_path: str):\n        self.model_path = model_path\n        self.model = None\n        self.start_time = time.time()\n        self.prediction_count = 0\n    \n    async def load_model(self):\n        \"\"\"Load ML/LLM model.\"\"\"\n        # TODO: Implement model loading\n        pass\n    \n    async def predict(self, input_data: Dict) -> Dict:\n        \"\"\"Make prediction.\"\"\"\n        # TODO: Implement prediction logic\n        pass\n    \n    def get_model_info(self) -> Dict:\n        \"\"\"Get model information.\"\"\"\n        # TODO: Implement model info retrieval\n        pass\n\n# Initialize service\nmodel_service = AIModelService(\"models/model.pkl\")\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup.\"\"\"\n    # TODO: Implement startup logic\n    await model_service.load_model()\n\n@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    # TODO: Implement health check\n    pass\n\n@app.post(\"/v1/predict\", response_model=PredictionResponse)\nasync def predict(request: PredictionRequest):\n    \"\"\"Prediction endpoint.\"\"\"\n    # TODO: Implement prediction endpoint\n    # 1. Validate input\n    # 2. Make prediction\n    # 3. Return response with timing\n    pass\n\n@app.get(\"/v1/model/info\")\nasync def model_info():\n    \"\"\"Get model information.\"\"\"\n    # TODO: Implement model info endpoint\n    pass\n\n# Test the API\n# Run with: uvicorn script:app --reload\n# TODO: Test endpoints using curl or httpie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 2: Dockerization\n\n### Why Docker for AI?\n\n**Benefits:**\n- Consistent environments (dev = prod)\n- Dependency isolation\n- Easy scaling\n- Version control for infrastructure\n- Portable across clouds\n\n### Docker Best Practices for AI\n1. Use multi-stage builds\n2. Minimize image size\n3. Cache dependencies\n4. Use specific base images\n5. Don't include training data in images\n\n### TODO 2.1: Create Docker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for AI service\ndockerfile_content = '''\n# TODO: Complete the Dockerfile\nFROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Run application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n'''\n\n# TODO: Create docker-compose.yml for local development\ndocker_compose_content = '''\nversion: '3.8'\n\nservices:\n  ai-service:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_PATH=/models/model.pkl\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./models:/models\n      - ./logs:/logs\n    restart: unless-stopped\n  \n  # TODO: Add monitoring services (Prometheus, Grafana)\n  \n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n'''\n\n# Save files\n# with open('Dockerfile', 'w') as f:\n#     f.write(dockerfile_content)\n# with open('docker-compose.yml', 'w') as f:\n#     f.write(docker_compose_content)\n\nprint(\"Docker configuration created!\")\nprint(\"\\nTo build: docker build -t ai-service .\")\nprint(\"To run: docker-compose up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 3: Logging & Monitoring\n\n### Production Logging Strategy\n\n**Log Levels:**\n- **DEBUG**: Detailed diagnostic info\n- **INFO**: General informational messages\n- **WARNING**: Warning messages\n- **ERROR**: Error messages\n- **CRITICAL**: Critical issues\n\n### What to Log\n1. **Request/Response**: Inputs, outputs, timing\n2. **Errors**: Exceptions with context\n3. **Performance**: Latency, throughput\n4. **Model**: Predictions, confidence scores\n5. **System**: Resource usage\n\n### TODO 3.1: Implement Production Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\nimport json\nfrom datetime import datetime\nfrom typing import Dict, Any\nimport sys\n\nclass ProductionLogger:\n    \"\"\"Production-grade logging system.\"\"\"\n    \n    def __init__(self, service_name: str, log_level: str = \"INFO\"):\n        self.service_name = service_name\n        self.logger = self._setup_logger(log_level)\n    \n    def _setup_logger(self, log_level: str) -> logging.Logger:\n        \"\"\"Setup structured logging.\"\"\"\n        # TODO: Implement logger setup\n        # Use JSON formatter for structured logs\n        pass\n    \n    def log_request(self, request_id: str, endpoint: str, params: Dict):\n        \"\"\"Log incoming request.\"\"\"\n        # TODO: Implement request logging\n        pass\n    \n    def log_prediction(self, request_id: str, prediction: Any, confidence: float, latency_ms: float):\n        \"\"\"Log prediction details.\"\"\"\n        # TODO: Implement prediction logging\n        pass\n    \n    def log_error(self, request_id: str, error: Exception, context: Dict):\n        \"\"\"Log error with context.\"\"\"\n        # TODO: Implement error logging\n        pass\n    \n    def log_performance(self, metrics: Dict):\n        \"\"\"Log performance metrics.\"\"\"\n        # TODO: Implement performance logging\n        pass\n\nclass MetricsCollector:\n    \"\"\"Collects and exposes metrics for monitoring.\"\"\"\n    \n    def __init__(self):\n        self.metrics: Dict[str, List[float]] = {\n            \"request_count\": [],\n            \"latency_ms\": [],\n            \"error_count\": [],\n            \"prediction_confidence\": []\n        }\n    \n    def record_request(self, latency_ms: float, success: bool):\n        \"\"\"Record request metrics.\"\"\"\n        # TODO: Implement request recording\n        pass\n    \n    def record_prediction(self, confidence: float):\n        \"\"\"Record prediction metrics.\"\"\"\n        # TODO: Implement prediction recording\n        pass\n    \n    def get_metrics_summary(self) -> Dict:\n        \"\"\"Get summary of metrics.\"\"\"\n        # TODO: Implement metrics summary\n        pass\n    \n    def export_prometheus_format(self) -> str:\n        \"\"\"Export metrics in Prometheus format.\"\"\"\n        # TODO: Implement Prometheus export\n        pass\n\n# Test logging\n# logger = ProductionLogger(\"ai-service\")\n# logger.log_request(\"req-123\", \"/v1/predict\", {\"input\": \"test\"})\n# logger.log_prediction(\"req-123\", \"result\", 0.95, 45.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 4: Model Drift Detection\n\n### What is Model Drift?\n\n**Types of Drift:**\n1. **Data Drift**: Input distribution changes\n2. **Concept Drift**: Relationship between input/output changes\n3. **Upstream Drift**: Data pipeline changes\n\n### Detection Methods\n- Statistical tests (KS test, Chi-square)\n- Distribution comparison\n- Performance monitoring\n- A/B testing\n\n### TODO 4.1: Implement Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nfrom scipy import stats\nfrom typing import List, Dict, Tuple\nfrom collections import deque\n\nclass DriftDetector:\n    \"\"\"Detects data and concept drift in production.\"\"\"\n    \n    def __init__(self, reference_data: np.ndarray, window_size: int = 1000):\n        self.reference_data = reference_data\n        self.window_size = window_size\n        self.production_window = deque(maxlen=window_size)\n        self.drift_history: List[Dict] = []\n    \n    def add_production_sample(self, sample: np.ndarray):\n        \"\"\"Add new production sample.\"\"\"\n        # TODO: Implement sample addition\n        pass\n    \n    def detect_data_drift(self, significance_level: float = 0.05) -> Tuple[bool, float]:\n        \"\"\"Detect data drift using statistical test.\"\"\"\n        # TODO: Implement drift detection\n        # Use Kolmogorov-Smirnov test or similar\n        pass\n    \n    def compute_drift_score(self) -> float:\n        \"\"\"Compute drift score (0-1).\"\"\"\n        # TODO: Implement drift score computation\n        pass\n    \n    def get_drift_report(self) -> Dict:\n        \"\"\"Generate drift report.\"\"\"\n        # TODO: Implement drift reporting\n        pass\n\nclass PerformanceMonitor:\n    \"\"\"Monitors model performance over time.\"\"\"\n    \n    def __init__(self, baseline_metrics: Dict):\n        self.baseline_metrics = baseline_metrics\n        self.performance_history: List[Dict] = []\n        self.alert_thresholds = {\n            \"accuracy_drop\": 0.05,\n            \"latency_increase\": 2.0\n        }\n    \n    def record_performance(self, metrics: Dict):\n        \"\"\"Record performance metrics.\"\"\"\n        # TODO: Implement performance recording\n        pass\n    \n    def detect_degradation(self) -> List[str]:\n        \"\"\"Detect performance degradation.\"\"\"\n        # TODO: Implement degradation detection\n        pass\n    \n    def should_trigger_alert(self) -> bool:\n        \"\"\"Check if alert should be triggered.\"\"\"\n        # TODO: Implement alert logic\n        pass\n\n# Test drift detection\n# reference = np.random.normal(0, 1, 1000)\n# detector = DriftDetector(reference)\n# for i in range(100):\n#     # Simulate drift\n#     sample = np.random.normal(0.5, 1, 1)\n#     detector.add_production_sample(sample)\n# drift_detected, p_value = detector.detect_data_drift()\n# print(f\"Drift detected: {drift_detected}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 5: Cloud Deployment\n\n### Deployment Options\n\n**Cloud Providers:**\n- AWS (SageMaker, EC2, Lambda)\n- Google Cloud (Vertex AI, Cloud Run)\n- Azure (ML, Container Instances)\n- Hugging Face Spaces\n\n### Deployment Strategies\n1. **Blue-Green**: Two identical environments\n2. **Canary**: Gradual rollout\n3. **Rolling**: Update incrementally\n4. **A/B Testing**: Compare versions\n\n### TODO 5.1: Deploy to Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment configuration examples\n\n# AWS deployment with Terraform/CloudFormation\naws_deployment_config = '''\n# TODO: Create deployment configuration\n# Example: Deploy to AWS ECS/Fargate\n\nresource \"aws_ecs_service\" \"ai_service\" {\n  name            = \"ai-service\"\n  cluster         = aws_ecs_cluster.main.id\n  task_definition = aws_ecs_task_definition.ai_service.arn\n  desired_count   = 2\n  \n  load_balancer {\n    target_group_arn = aws_lb_target_group.ai_service.arn\n    container_name   = \"ai-service\"\n    container_port   = 8000\n  }\n}\n'''\n\n# Kubernetes deployment\nk8s_deployment_config = '''\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ai-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ai-service\n  template:\n    metadata:\n      labels:\n        app: ai-service\n    spec:\n      containers:\n      - name: ai-service\n        image: your-registry/ai-service:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: MODEL_PATH\n          value: \"/models/model.pkl\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n'''\n\nclass DeploymentManager:\n    \"\"\"Manages deployment to cloud platforms.\"\"\"\n    \n    def __init__(self, platform: str):\n        self.platform = platform\n        self.deployment_history: List[Dict] = []\n    \n    def deploy(self, image_tag: str, config: Dict) -> str:\n        \"\"\"Deploy service to cloud.\"\"\"\n        # TODO: Implement deployment logic\n        pass\n    \n    def rollback(self, deployment_id: str):\n        \"\"\"Rollback to previous deployment.\"\"\"\n        # TODO: Implement rollback\n        pass\n    \n    def health_check(self, endpoint: str) -> bool:\n        \"\"\"Check deployment health.\"\"\"\n        # TODO: Implement health check\n        pass\n\nprint(\"Deployment configurations created!\")\nprint(\"Review and customize for your cloud provider.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 6: Building the Complete Production Service\n\n### System Architecture\n\n```\nLoad Balancer\n     |\n     v\nAPI Gateway (FastAPI)\n     |\n     +-- Model Service\n     +-- Logging\n     +-- Metrics Collection\n     +-- Drift Detection\n     +-- Cache (Redis)\n```\n\n### Production Checklist\n- [ ] API with proper error handling\n- [ ] Docker containerization\n- [ ] Structured logging\n- [ ] Metrics collection\n- [ ] Drift detection\n- [ ] Health checks\n- [ ] Auto-scaling\n- [ ] Monitoring dashboard\n\n### TODO 6.1: Integrate All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, BackgroundTasks\nimport asyncio\n\nclass ProductionAIService:\n    \"\"\"Complete production AI service.\"\"\"\n    \n    def __init__(self, config: Dict):\n        self.config = config\n        self.app = FastAPI(title=\"Production AI Service\")\n        self.model_service = AIModelService(config['model_path'])\n        self.logger = ProductionLogger(\"ai-service\")\n        self.metrics = MetricsCollector()\n        self.drift_detector = None  # Initialize with reference data\n        self._setup_routes()\n    \n    def _setup_routes(self):\n        \"\"\"Setup all API routes.\"\"\"\n        # TODO: Setup all endpoints\n        pass\n    \n    async def startup(self):\n        \"\"\"Initialize service on startup.\"\"\"\n        # TODO: Implement startup sequence\n        # 1. Load model\n        # 2. Initialize monitoring\n        # 3. Load reference data for drift detection\n        # 4. Start background tasks\n        pass\n    \n    async def background_monitoring(self):\n        \"\"\"Background task for monitoring.\"\"\"\n        # TODO: Implement background monitoring\n        # Periodically check drift, performance, etc.\n        pass\n    \n    async def process_prediction_request(self, request: PredictionRequest) -> PredictionResponse:\n        \"\"\"Process prediction with full monitoring.\"\"\"\n        # TODO: Implement end-to-end request processing\n        # 1. Log request\n        # 2. Make prediction\n        # 3. Record metrics\n        # 4. Check drift\n        # 5. Log response\n        # 6. Return result\n        pass\n    \n    def get_service_status(self) -> Dict:\n        \"\"\"Get comprehensive service status.\"\"\"\n        # TODO: Implement status reporting\n        pass\n\n# Initialize production service\n# config = {\n#     'model_path': 'models/model.pkl',\n#     'log_level': 'INFO',\n#     'drift_detection_enabled': True\n# }\n# service = ProductionAIService(config)\n# Run with: uvicorn service:service.app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Summary and Next Steps\n\n### What You've Learned\n- Building production APIs with FastAPI\n- Dockerizing AI applications\n- Production logging and monitoring\n- Drift detection and performance monitoring\n- Cloud deployment strategies\n- Building complete production AI services\n\n### Next Week Preview\nWeek 12 will cover the **Capstone System**, where you'll:\n- Design end-to-end AI agent platforms\n- Handle failure scenarios\n- Implement reliability patterns\n- Build a production-ready AI agent platform\n\n### Further Practice\n1. Deploy your service to a cloud provider\n2. Set up monitoring dashboards (Grafana)\n3. Implement A/B testing\n4. Add authentication and rate limiting\n5. Create CI/CD pipelines\n\n---\n\n**Great job on completing Week 11!** \ud83c\udf89"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}